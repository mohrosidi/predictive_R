---
title: "Data Spliting dan Feature Engineering"
author: "Moh. Rosidi"
date: "7/23/2020"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
    df_print: paged
    theme: yeti
    highlight: textmate
    css: assets/style.css
  pdf_document:
    toc: yes
    toc_depth: '3'
    latex_engine: xelatex
---

# Dataset Ames

Sebuah dataset terkait data properti yang ada di Ames IA. Dataset ini memiliki 82 variabel dan 2930 baris. Untuk informasi lebih lanjut terkait dataset ini, kunjungin tautan berikut:

* <https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt>
* <http://ww2.amstat.org/publications/jse/v19n3/decock.pdf>

# Persiapan {.tabset}

## Library

Terdapat beberapa paket yang digunakan dalam pembuatan model prediktif menggunakan *tree based algorithm*. Paket-paket ditampilkan sebagai berikut:

```{r import-lib}
library(rsample)
library(tidyverse)
library(recipes)
library(caret)
library(AmesHousing)
library(janitor)
library(DataExplorer)
library(skimr)
```

1. `rsample` : membantu proses *data splitting*
2. `tidyverse` : kumpulan paket dalam bidang data science
3. `recipes`: membantu proses data *preprocessing*
4. `caret` : berisikan sejumlah fungsi yang dapat merampingkan proses pembuatan model regresi dan klasifikasi
5. `AmesHousing` : berisikan dataset Ames
6. `janitor` : memperbaiki format kolom
7. `DataExplorer` : membantu EDA
8. `skimr` : membuat ringkasan data

## Import Dataset

Import dataset dilakukan dengan menggunakan fungsi `data()`. Fungsi ini digunakan untuk mengambil data yang ada dalam sebuah paket.

```{r import-data}
data("ames_raw")
```

Untuk melihat struktur datanya, jalankan sintaks berikut:

```{r}
glimpse(ames_raw)

```

Berdasarkan output yang dihasilkan, nama kolom pada dataset tersebut perlu dilakukan *cleaning* agar tidak mengandung koma.

```{r}
ames <- ames_raw %>%
  janitor::clean_names() %>%
  mutate(across(where(is.character), as.factor))
```

Untuk mengecek hasilnya jalankan kembali fungsi `glimpse()`.

```{r}
glimpse(ames)
```


# Data Splitting

Proses *data splitting* dilakukan setelah data di import ke dalam sistem. Hal ini dilakukan untuk memastikan tidak adanya kebocoran data yang mempengaruhi proses pembuatan model. Data dipisah menjadi dua buah set, yaitu: *training* dan *test*. Data *training* adalah data yang akan kita gunakan untuk membentuk model. Seluruh proses sebelum uji model akan menggunakan data *training*. Proses tersebut, antara lain: EDA, *feature engineering*, dan validasi silang. Data *test* hanya digunakan saat kita akan menguji performa model dengan data baru yang belum pernah dilihat sebelumnya.

Rekomendasi yang umum digunakan pada pemisahan data adalah 60% sampai dengan 80% data *training* dan sisanya adalah data *test*. Proporsi tersebut merupakan panduan umum dalam pemisahan data. Namun, beberapa hal perlu dipertimbangkan dalam menentukan proporsi pemisahan yang digunakan, antara lain:

* Data *training* terlalu besar (misal: > 80%) tidak memungkinkan kita untuk membuat pengujian performa model prediktif dengan cukup baik. Kita mungkin akan membuat model yang cukup baik untuk data *training* kita, namun tidak dapat mengeneralisasi pola dalam data dengan cukup baik (*overfitting*)
* Data *test* terlalu besar (misal: > 40%) tidak memungkinkan kita membuat pengujian yang baik terhadap parameter model (*underfitting*).

Terdapat dua buah jenis sampling pada tahapan *data splitting*, yaitu:

1. *simple random sampling* : sampling acak tanpa mempertimbangkan adanya strata dalam data
2. *stratified random sampling* : sampling dengan memperhatikan strata dalam sebuah variabel.

Berikut ditampilkan dua buah macam pemisahan data tersebut.

**_Simple Random Sampling_**

```{r data-split}
set.seed(123)

split1  <- initial_split(ames, prop = 0.7)
ames_train1  <- training(split1)
ames_test1   <- testing(split1)
```

**_Stratified Random Sampling_**

```{r data-split}
set.seed(123)

split2  <- initial_split(ames, prop = 0.7, strata = "sale_price")
ames_train2  <- training(split2)
ames_test2   <- testing(split2)
```

Untuk mengecek distribusi dari kedua set data, kita dapat mevisualisasikan distribusi dari variabel target pada kedua set tersebut.

```{r target-vis}
# training set
ames_train1 %>%
  mutate(variant = "Simple") %>%
  bind_rows({
    ames_train2 %>%
      mutate(variant = "Stratified")
  }) %>%
  ggplot(aes(x = sale_price, color = variant)) + 
  geom_density() 
# test set
ames_test1 %>%
  mutate(variant = "Simple") %>%
  bind_rows({
    ames_test2 %>%
      mutate(variant = "Stratified")
  }) %>%
  ggplot(aes(x = sale_price, color = variant)) + 
  geom_density() 
```

# Analisis Data Eksploratif

Analsiis data eksploratif (EDA) ditujukan untuk mengenali data sebelum kita menentukan algoritma yang cocok digunakan untuk menganalisa data lebih lanjut. EDA merupakan sebuah proses iteratif yang secara garis besar menjawab beberapa pertanyaan umum, seperti:

1. Bagaimana distribusi data pada masing-masing variabel?
2. Apakah terdapat asosiasi atau hubungan antar variabel dalam data?

## Ringkasan Data

Terdapat dua buah fungsi yang digunakan dalam membuat ringkasan data, antara lain:

1. `skim()` : fungsi dari paket `skimr` untuk membuat ringkasan data yang lebih detail dibanding `glimpse()`, seperti: statistika deskriptif masing-masing kolom, dan informasi *missing value* dari masing-masing kolom.
2. `plot_missing()` : fungsi untuk memvisualisasikan persentase *missing value* pada masing-masing variabel atau kolom data

```{r}
skim(ames_train2)
```

```{r}
mv_vis <- plot_missing(ames_train2)
```

Pada dataset, terdapat 2053 baris dan 82 kolom. Pada data terdapat *missing value* pada sejumlah kolom dengan *rate* yang berbeda-beda. Terdapat beberapa pendekan yang dapat digunakan untuk menangani *missing value*, antara lain:

1, **Membuang kolom atau baris data**. Membuang kolom data pada umumnya dilakukan jika *missing value* pada kolom tersebut > 50%-70%. Dalam hal ini kolom dengan rekomendasi `remove` berdasarkan visualisasi data yang dihasilkan. Membuang baris, dilakukan apabila.proprosi *missing value* cukup kecil (misal <10%) dan tidak mempengaruhi secara signifikan jumlah training data yang digunakan.
2. **Imputasi**. Proses imputasi merupakan proses mengisi *missing value* dengan sejumlah pendekatan. Pendekatan yang digunakan, antara lain:

* Nilai pusat (mis: mean, median, modus)
* Hasil prediksi model (mis: knn, *tree-based model*)

Perlu diperhatikan, pendekatan nilai pusat menghiraukan adanya pola yang terbentuk dalam data. Sebagai gambaran, perhatikan visualisasi di bawah ini yang membandingkan hasil imputasi menggunakan model maupun pendekatan nilai pusat.

![Perbandingan tiga buah metode imputasi](https://bradleyboehmke.github.io/HOML/03-engineering_files/figure-html/engineering-imputation-examples-1.png)

Variabel dengan proporsi *missing value* > 40%, antara lain:

```{r}
var_miss <-
  mv_vis$data %>%
  filter(pct_missing > 0.4) %>%
  dplyr::select(feature) %>%
  pull()

var_miss
```


## Varian

Untuk melihat varian masing-masing variabel, kita dapat mevisualisasikan data menggunakan histogram dan barplot.

```{r}
plot_histogram(ames_train2, ncol = 2L, nrow = 2L)
```

Berdasarkan hasil visualisasi histogram pada data numerik, sebagian besar data tidak berdistribusi normal atau tidak simestris.

```{r}
plot_bar(ames_train2, ncol=2L, nrow = 2L)
```

Sejumlah kolom pada variabel kategorikal memiliki varian yang mendekati nol. Untuk mengetahui kolom dengan varian mendekati nol atau nol, jalankan sintaks berikut:

```{r}
caret::nearZeroVar(ames_train2, saveMetrics = TRUE) %>% 
  tibble::rownames_to_column() %>% 
  filter(nzv)
```

Setidaknya terdapat 19 kolom dengan varian mendekati nolo atau nol. Membuat variabel dengan varian mendekati nol akan menurunkan dimensi data.

## Kovarian

Kovarian dapat divisualisasikan dengan menggunakan heatmap yang mengkode nilai koefisien korelasi berdasarkan warna.

```{r}
plot_correlation(ames_train2)
```


# Target and Feature Engineering

*Data preprocessing* dan *engineering* mengacu pada proses penambahan, penghapusan, atau transformasi data. Waktu yang diperlukan untuk memikirkan identifikasi kebutuhan *data engineering* dapat berlangsung cukup lama dan proprsinya akan menjadi yang terbesar dibandingkan analisa lainnya. Hal ini disebabkan karena kita perlu untuk memahami data apa yang akan kita oleh atau diinputkan ke dalam model.

Untuk menyederhanakan proses *feature engineerinh*, kita harus memikirkannya sebagai sebuah *blueprint* dibanding melakukan tiap tugasnya secara satu persatu. Hal ini membantu kita dalam dua hal:

1. Berpikir secara berurutan
2. Mengaplikasikannya secara tepat selama proses *resampling*

## Urutan Langkah-Langkah Feature Engineering

Memikirkan *feature engineering* sebagai sebuah *blueprint* memaksa kita untuk memikirkan urutan langkah-langkah *preprocessing* data. Meskipun setiap masalah mengharuskan kita untuk memikirkan efek *preprocessing* berurutan, ada beberapa saran umum yang harus kita pertimbangkan:

* Jika menggunakan log atau transformasi Box-Cox, jangan memusatkan data terlebih dahulu atau melakukan operasi apa pun yang dapat membuat data menjadi tidak positif. Atau, gunakan transformasi Yeo-Johnson sehingga kita tidak perlu khawatir tentang hal ini.
* *One-hot* atau *dummy encoding* biasanya menghasilkan data jarang (*sparse*) yang dapat digunakan oleh banyak algoritma secara efisien. Jika kita menstandarisasikan data tersebut, kita akan membuat data menjadi padat (*dense*) dan kita kehilangan efisiensi komputasi. Akibatnya, sering kali lebih disukai untuk standardisasi fitur numerik kita dan kemudian *one-hot/dummy endode*.
* Jika kila mengelompokkan kategori (*lumping*) yang jarang terjadi  secara bersamaan, lakukan sebelum *one-hot/dummy endode*.
* Meskipun kita dapat melakukan prosedur pengurangan dimensi pada fitur-fitur kategorikal, adalah umum untuk melakukannya terutama pada fitur numerik ketika melakukannya untuk tujuan rekayasa fitur.

Sementara kebutuhan proyek kita mungkin beragam, berikut ini adalah urutan langkah-langkah potensial yang disarankan untuk sebagian besar masalah:

1. Filter fitur dengan varians nol (*zero varians*) atau hampir nol (*near zero varians*).
2. Lakukan imputasi jika diperlukan.
3. Normalisasi untuk menyelesaikan *skewness* fitur numerik.
4. Standardisasi fitur numerik (*centering* dan *scaling*).
5. Lakukan reduksi dimensi (mis., PCA) pada fitur numerik.
6. *one-hot/dummy endode* pada fitur kategorikal.

## Meletakkan Seluruh Proses Secara Bersamaan

Untuk mengilustrasikan bagaimana proses ini bekerja bersama menggunakan R, mari kita lakukan penilaian ulang sederhana pada set data `ames` yang kita gunakan  dan lihat apakah beberapa *feature engineering* sederhana meningkatkan kemampuan prediksi model kita. Tapi pertama-tama, kita berkenalan dengat paket `recipe`.

Paket `recipe` ini memungkinkan kita untuk mengembangkan *blueprint feature engineering* secara berurutan. Gagasan di balik `recipe` mirip dengan `caret :: preProcess()` di mana kita ingin membuat *blueprint preprocessing* tetapi menerapkannya nanti dan dalam setiap resample.

Ada tiga langkah utama dalam membuat dan menerapkan rekayasa fitur dengan `recipe`:

1. `recipe()`: tempat kita menentukan langkah-langkah rekayasa fitur untuk membuat *blueprint*.
2. `prep()`: memperkirakan parameter *feature engineering* berdasarkan data *training*.
3. `bake()`: terapkan *blueprint* untuk data baru.

Langkah pertama adalah di mana kita menentukan *blueprint*. Dengan proses ini, Kita memberikan formula model yang ingin kita buat (variabel target, fitur, dan data yang menjadi dasarnya) dengan fungsi `recipe()` dan kemudian kita secara bertahap menambahkan langkah-langkah rekayasa fitur dengan fungsi `step_xxx()`. 

*Blueprint* yang akan dilakukan dalam proses *feature engineering* berdasarkan hasil analisis data eksploratif, antara lain:

1. Memfilter variabel kategorikal dengan varian mendekati nol
2. Membuang variabel dengan *missing value* > 40% dan ID
3. Melakukan imputasi menggunakan knn pada variabel dengan *missing value*
4. Melakukan *categorical encoding* pada variabel kategorikal dengan kelas > 10
5. Standardisasi data numerik
6. Reduksi dimensi menggunakan pca
7. *dummy encoding* pada data kategorikal

```{r preprocess}

blueprint <- recipe(sale_price ~., data = ames_train2) %>%
  # feature filtering
  step_nzv(all_nominal()) %>%
  # remove feature
  step_rm(var_miss, pid) %>%
  # imputation
  step_knnimpute(all_predictors(), neighbors = 6) %>%
  # label encoding
  step_integer(ms_sub_class, neighborhood, exterior_1st, exterior_2nd, sale_type) %>%
  # standardization
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  # feature extraction
  step_pca(all_numeric(), -all_outcomes(), threshold = .7) %>%
  # one hot/dummy encoding
  step_dummy(all_nominal())

blueprint
```

Selanjutnya, *blueprint* yang telah dibuat dilakukan *training* pada data *training*. Perlu diperhatikan, kita tidak melakukan proses *training* pada data *test* untuk mencegah *data leakage*.

```{r prep}
prepare <- prep(blueprint, training = ames_train2)
prepare
```

Langkah terakhir adalah mengaplikasikan *blueprint* pada data *training* dan *test* menggunakan fungsi `bake()`.

```{r baked}
baked_train <- bake(prepare, new_data = ames_train2)
baked_test <- bake(prepare, new_data = ames_test2)
baked_train
```

```{r}
skim(baked_train)
```
