---
title: "Gradient Boosting"
author: "Moh. Rosidi"
date: "7/22/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Dataset Spotify

Pada artikel ini, kita akan membuat model prediktif pada dataset `Spotify`. `Spotify`  Merupakan dataset yang berisikan daftar lagu dan fitur audio dari band/penyanyi ternama dunia, antara lain: Queens, Maroon 5, dan Jason Mraz.

Kolom-kolom pada dataset tersebut, antara lain:

* `id` : nomor seri lagu
*  `name` : judul lagu
* `popularity` : tingkat popularitas lagu
* `album.id` : nomor seri album
* `album.name` : nama album
* `album.total_tracks` : total lagu dalam album
* `track_number` : nomor lagu dalam album
* `duration_ms` : durasi lagu dalam satuan ms
* `danceability` : elemen musik termasuk tempo, stabilitas ritme, kekuatan beat, dan keteraturan keseluruhan. Nilai 0,0 paling tidak bisa digunakan untuk menari dan 1,0 paling bisa digunakan untuk menari.
* `energy` : Energi adalah ukuran dari 0,0 hingga 1,0 dan mewakili ukuran persepsi intensitas dan aktivitas. Biasanya, trek yang energik terasa cepat, keras, dan berisik. Sebagai contoh, death metal memiliki energi tinggi, sedangkan prelude Bach mendapat skor rendah pada skala. Fitur perseptual yang berkontribusi pada atribut ini meliputi rentang dinamis, persepsi kenyaringan, warna nada, onset rate, dan entropi umum.
* `key` : Kunci dari trek adalah. Integer memetakan ke pitch menggunakan notasi Pitch Class standar. Misalnya. 0 = C, 1 = C♯ / D ♭, 2 = D, dan seterusnya.
* `loudness` : Keseluruhan kenyaringan trek dalam desibel (dB). Nilai kenyaringan rata-rata di seluruh trek dan berguna untuk membandingkan kenyaringan relatif trek. Kenyaringan adalah kualitas suara yang merupakan korelasi psikologis utama dari kekuatan fisik (amplitudo). Nilai kisaran khas antara -60 dan 0 db.
* `mode` : Mode menunjukkan modalitas (besar atau kecil) dari suatu trek, jenis skala dari mana konten melodinya diturunkan. Mayor diwakili oleh 1 dan minor adalah 0.
* `speechiness` : Speechiness mendeteksi keberadaan kata-kata yang diucapkan di trek. Semakin eksklusif pidato-seperti rekaman (mis. Acara bincang-bincang, buku audio, puisi), semakin dekat dengan 1.0 nilai atribut. Nilai di atas 0,66 menggambarkan trek yang mungkin seluruhnya terbuat dari kata-kata yang diucapkan. Nilai antara 0,33 dan 0,66 menggambarkan trek yang mungkin berisi musik dan ucapan, baik dalam bagian atau lapisan, termasuk kasus-kasus seperti musik rap. Nilai di bawah 0,33 kemungkinan besar mewakili musik dan trek non-ucapan lainnya.
* `acousticness` : Ukuran kepercayaan dari 0,0 hingga 1,0 dari apakah trek akustik. 1.0 mewakili kepercayaan tinggi trek adalah akustik.
* `instrumentalness` : Memprediksi apakah suatu lagu tidak mengandung vokal. Suara “Ooh” dan “aah” diperlakukan sebagai instrumen dalam konteks ini. Rap atau trek kata yang diucapkan jelas "vokal". Semakin dekat nilai instrumentalness ke 1.0, semakin besar kemungkinan trek tidak mengandung konten vokal. Nilai di atas 0,5 dimaksudkan untuk mewakili trek instrumental, tetapi kepercayaan diri lebih tinggi ketika nilai mendekati 1.0.
* `liveness` : Mendeteksi keberadaan audiens dalam rekaman. Nilai liveness yang lebih tinggi mewakili probabilitas yang meningkat bahwa trek dilakukan secara langsung. Nilai di atas 0,8 memberikan kemungkinan kuat bahwa trek live.
* `valence` : Ukuran 0,0 hingga 1,0 yang menggambarkan kepositifan musik yang disampaikan oleh sebuah trek. Lagu dengan valensi tinggi terdengar lebih positif (mis. Bahagia, ceria, gembira), sedangkan trek dengan valensi rendah terdengar lebih negatif (mis. Sedih, tertekan, marah).
* `tempo` : Perkiraan tempo trek secara keseluruhan dalam beat per menit (BPM). Dalam terminologi musik, tempo adalah kecepatan atau kecepatan dari bagian yang diberikan dan diturunkan langsung dari durasi beat rata-rata.
* `time_signature` : An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).

# Persiapan {.tabset}

## Library

Terdapat beberapa paket yang digunakan dalam pembuatan model prediktif menggunakan *gradient boost*. Paket-paket yang digunakan ditampilkan sebagai berikut:

```{r import-lib, cache=TRUE}
# library pembantu
library(e1071)
library(rsample)
library(recipes)
library(DataExplorer)
library(skimr)
library(DMwR)
library(MLmetrics)
library(tidyverse)

# library model
library(caret) 
library(gbm)


# paket penjelasan model
library(rpart.plot)  
library(vip)
library(pdp)
```

**Paket Pembantu**

1.  `e1071` : paket dengan sejumlah fungsi untuk melakukan *latent class analysis, short time Fourier transform, fuzzy clustering, support vector machines, shortest path computation, bagged clustering, naive Bayes classifier*, dll. Paket ini merupakan paket pembantu dalam proses *fitting* sejumlah model pohon
2. `tidyverse` : kumpulan paket dalam bidang data science
3. `rsample` : membantu proses *data splitting*
4. `recipes`: membantu proses data pra-pemrosesan
5. `DataExplorer` : EDA
6. `skimr` : membuat ringkasan data
7. `DMwR` : paket untuk melakukan sampling "smote"

**Paket untuk Membangun Model**

1. `caret` : berisikan sejumlah fungsi yang dapat merampingkan proses pembuatan model regresi dan klasifikasi
2. `gbm` : membentuk model *gradient boost*


**Paket Interpretasi Model**

1. `rpart.plot` : visualisasi *decision trees*
2. `vip` : visualisasi *variable importance*
3. `pdp` : visualisasi plot ketergantungan parsial

## Import Dataset

Import dataset dilakukan dengan menggunakan fungsi `readr()`. Fungsi ini digunakan untuk membaca file dengan ekstensi `.csv`.

```{r import-data, cache=TRUE}
spotify <- read_csv("data/spotify.csv")

# data cleaning
key_labs = c('c', 'c#', 'd', 'd#', 'e', 'f', 
             'f#', 'g', 'g#', 'a', 'a#', 'b')
mode_labs = c('minor', 'major')

spotify <- spotify %>%
  select(popularity, duration_ms:artist) %>%
  mutate(time_signature = factor(time_signature),
         key = factor(key, labels = key_labs),
         mode = factor(mode, labels = mode_labs),
         artist = factor(artist, labels = c("Jason_Mraz", "Maroon_5", "Queen" )))
```


# Data Splitting

Proses *data splitting* dilakukan setelah data di import ke dalam sistem. Hal ini dilakukan untuk memastikan tidak adanya kebocoran data yang mempengaruhi proses pembuatan model. Data dipisah menjadi dua buah set, yaitu: *training* dan *test*. Data *training* adalah data yang akan kita gunakan untuk membentuk model. Seluruh proses sebelum uji model akan menggunakan data *training*. Proses tersebut, antara lain: EDA, *feature engineering*, dan validasi silang. Data *test* hanya digunakan saat kita akan menguji performa model dengan data baru yang belum pernah dilihat sebelumnya.

Terdapat dua buah jenis sampling pada tahapan *data splitting*, yaitu:

1. *random sampling* : sampling acak tanpa mempertimbangkan adanya strata dalam data
2. *startified random sampling* : sampling dengan memperhatikan strata dalam sebuah variabel.

Dalam proses pembentukan model kali ini, kita akan menggunakan metode kedua dengan tujuan untuk memperoleh distribusi yang seragam dari variabel target (`artist`).

```{r data-split, cache=TRUE}
set.seed(123)

split  <- initial_split(spotify, prop = 0.8, strata = "artist")
data_train  <- training(split)
data_test   <- testing(split)
```

Untuk mengecek distribusi dari kedua set data, kita dapat mevisualisasikan distribusi dari variabel target pada kedua set tersebut.

```{r target-vis, cache=TRUE}
# training set
ggplot(data_train, aes(x = artist)) + 
  geom_bar() 
# test set
ggplot(data_test, aes(x = artist)) + 
  geom_bar() 
```


# Analisis Data Eksploratif

Analsiis data eksploratif (EDA) ditujukan untuk mengenali data sebelum kita menentukan algoritma yang cocok digunakan untuk menganalisa data lebih lanjut. EDA merupakan sebuah proses iteratif yang secara garis besar menjawab beberapa pertanyaan umum, seperti:

1. Bagaimana distribusi data pada masing-masing variabel?
2. Apakah terdapat asosiasi atau hubungan antar variabel dalam data?

## Ringkasan Data

Terdapat dua buah fungsi yang digunakan dalam membuat ringkasan data, antara lain:

1. `glimpse()`: varian dari `str()` untuk mengecek struktur data. Fungsi ini menampilkan transpose dari tabel data dengan menambahkan informasi, seperti: jenis data dan dimensi tabel.
2. `skim()` : fungsi dari paket `skimr` untuk membuat ringkasan data yang lebih detail dibanding `glimpse()`, seperti: statistika deskriptif masing-masing kolom, dan informasi *missing value* dari masing-masing kolom.
3. `plot_missing()` : fungsi untuk memvisualisasikan persentase *missing value* pada masing-masing variabel atau kolom data


```{r glimpse, cache=TRUE}
glimpse(data_train)
```

```{r skim, cache=TRUE}
skim(data_train)
```

```{r missing-vis, cache=TRUE}
plot_missing(data_train)
```

Berdasarkan ringkasan data yang dihasilkan, diketahui dimensi data sebesar 982 baris dan 15 kolom. Dengan rincian masing-masing kolom, yaitu: 4 kolom dengan jenis data factor dan 11 kolom dengan jenis data numeric. Informasi lain yang diketahui adalah seluruh kolom dalam data tidak memiliki *missing value*.

## Variasi

Variasi dari tiap variabel dapat divisualisasikan dengan menggunakan histogram (numerik) dan baplot (kategorikal).

```{r hist, cache=TRUE}
plot_histogram(data_train, ncol = 2L, nrow = 2L)
```

```{r bar, cache=TRUE}
plot_bar(data_train, ncol = 2L, nrow = 2L)
```

Berdasarkan hasil visualisasi diperoleh bahwa sebagian besar variabel numerik memiliki distribusi yang tidak simetris. Sedangkan pada variabel kategorikal diketahui bahwa seluruh variabel memiliki variasi yang tidak mendekati nol atau nol. Untuk mengetahui variabel dengan variasi mendekati nol atau nol, dapat menggunakan sintaks berikut:

```{r nzv, cache=TRUE}
nzvar <- nearZeroVar(data_train, saveMetrics = TRUE) %>% 
  rownames_to_column() %>% 
  filter(nzv)
nzvar
```

## Kovarian

Kovarian dapat dicek melalui visualisasi *heatmap* koefisien korelasi.

```{r heatmap, cache=TRUE}
plot_correlation(data_train, 
                 cor_args = list(method = "spearman"))
```

# Target and Feature Engineering

*Data preprocessing* dan *engineering* mengacu pada proses penambahan, penghapusan, atau transformasi data. Waktu yang diperlukan untuk memikirkan identifikasi kebutuhan *data engineering* dapat berlangsung cukup lama dan proprsinya akan menjadi yang terbesar dibandingkan analisa lainnya. Hal ini disebabkan karena kita perlu untuk memahami data apa yang akan kita oleh atau diinputkan ke dalam model.

Untuk menyederhanakan proses *feature engineerinh*, kita harus memikirkannya sebagai sebuah *blueprint* dibanding melakukan tiap tugasnya secara satu persatu. Hal ini membantu kita dalam dua hal:

1. Berpikir secara berurutan
2. Mengaplikasikannya secara tepat selama proses *resampling*

## Urutan Langkah-Langkah Feature Engineering

Memikirkan *feature engineering* sebagai sebuah *blueprint* memaksa kita untuk memikirkan urutan langkah-langkah *preprocessing* data. Meskipun setiap masalah mengharuskan kita untuk memikirkan efek *preprocessing* berurutan, ada beberapa saran umum yang harus kita pertimbangkan:

* Jika menggunakan log atau transformasi Box-Cox, jangan memusatkan data terlebih dahulu atau melakukan operasi apa pun yang dapat membuat data menjadi tidak positif. Atau, gunakan transformasi Yeo-Johnson sehingga kita tidak perlu khawatir tentang hal ini.
* *One-hot* atau *dummy encoding* biasanya menghasilkan data jarang (*sparse*) yang dapat digunakan oleh banyak algoritma secara efisien. Jika kita menstandarisasikan data tersebut, kita akan membuat data menjadi padat (*dense*) dan kita kehilangan efisiensi komputasi. Akibatnya, sering kali lebih disukai untuk standardisasi fitur numerik kita dan kemudian *one-hot/dummy endode*.
* Jika kila mengelompokkan kategori (*lumping*) yang jarang terjadi  secara bersamaan, lakukan sebelum *one-hot/dummy endode*.
* Meskipun kita dapat melakukan prosedur pengurangan dimensi pada fitur-fitur kategorikal, adalah umum untuk melakukannya terutama pada fitur numerik ketika melakukannya untuk tujuan rekayasa fitur.

Sementara kebutuhan proyek kita mungkin beragam, berikut ini adalah urutan langkah-langkah potensial yang disarankan untuk sebagian besar masalah:

1. Filter fitur dengan varians nol (*zero varians*) atau hampir nol (*near zero varians*).
2. Lakukan imputasi jika diperlukan.
3. Normalisasi untuk menyelesaikan *skewness* fitur numerik.
4. Standardisasi fitur numerik (*centering* dan *scaling*).
5. Lakukan reduksi dimensi (mis., PCA) pada fitur numerik.
6. *one-hot/dummy endode* pada fitur kategorikal.

## Meletakkan Seluruh Proses Secara Bersamaan

Untuk mengilustrasikan bagaimana proses ini bekerja bersama menggunakan R, mari kita lakukan penilaian ulang sederhana pada set data `ames` yang kita gunakan  dan lihat apakah beberapa *feature engineering* sederhana meningkatkan kemampuan prediksi model kita. Tapi pertama-tama, kita berkenalan dengat paket `recipe`.

Paket `recipe` ini memungkinkan kita untuk mengembangkan *bluprint feature engineering* secara berurutan. Gagasan di balik `recipe` mirip dengan `caret :: preProcess()` di mana kita ingin membuat *blueprint preprocessing* tetapi menerapkannya nanti dan dalam setiap resample.

Ada tiga langkah utama dalam membuat dan menerapkan rekayasa fitur dengan `recipe`:

1. `recipe()`: tempat kita menentukan langkah-langkah rekayasa fitur untuk membuat *blueprint*.
2. `prep()`: memperkirakan parameter *feature engineering* berdasarkan data *training*.
3. `bake()`: terapkan *blueprint* untuk data baru.

```{r}
blueprint <- recipe(artist ~ ., data = data_train) %>%
  step_nzv(all_nominal())  %>%
  
  # 2. imputation to missing value
  # step_medianimpute("<Num_Var_name>") %>% # median imputation
  # step_meanimpute("<Num_var_name>") %>% # mean imputation
  # step_modeimpute("<Cat_var_name>") %>% # mode imputation
  # step_bagimpute("<Var_name>") %>% # random forest imputation
  # step_knnimpute("<Var_name>") %>% # knn imputation
  
  # Label encoding for categorical variable with many classes 
  # step_integer("<Cat_var_name>") %>%
  
  # 3. normalize to resolve numeric feature skewness
  step_center(all_numeric(), -all_outcomes()) %>%
  
  # 4. standardize (center and scale) numeric feature
  step_scale(all_numeric(), -all_outcomes()) 
```

Selanjutnya, *blueprint* yang telah dibuat dilakukan *training* pada data *training*. Perlu diperhatikan, kita tidak melakukan proses *training* pada data *test* untuk mencegah *data leakage*.

```{r prep, cache=TRUE}
prepare <- prep(blueprint, training = data_train)
prepare
```

Langkah terakhir adalah mengaplikasikan *blueprint* pada data *training* dan *test* menggunakan fungsi `bake()`.

```{r baked, cache=TRUE}
baked_train <- bake(prepare, new_data = data_train)
baked_test <- bake(prepare, new_data = data_test)
baked_train
```

# Boosting

*Gradient boosted machines* (GBMs) adalah algoritma *machine learning* yang sangat populer yang telah terbukti berhasil di banyak domain dan merupakan salah satu metode utama untuk memenangkan kompetisi Kaggle. Sementara *random forest* membangun ansambel pohon independen yang dalam, GBM membangun ansambel pohon berturut-turut yang dangkal dan lemah dengan setiap pohon belajar dan meningkatkan pada sebelumnya. Ketika digabungkan, banyak pohon berturut-turut yang lemah ini menghasilkan "komite" yang kuat yang seringkali sulit dikalahkan dengan algoritma lain. 

Beberapa model *supervised machine learning* tersusun atas model prediksi tunggal (yaitu [regresi linier](http://uc-r.github.io/linear_regression), [penalized model](http://uc-r.github.io/regularized_regression), [naive bayes](http://uc-r.github.io/naive_bayes), [support vector machines](http://uc-r.github.io/svm)). Atau, pendekatan lain seperti [bagging](http://uc-r.github.io/regression_trees) dan [random forest](http://uc-r.github.io/random_forests) dibangun di atas gagasan membangun ansambel model di mana masing-masing model memprediksi hasil dan kemudian hasil prediksi dirata-rata (regresi) atau menggunakan sistem voting terbanyak (klasifikasi). Keluarga metode *boosting* didasarkan pada strategi konstruktif yang berbeda dari pembentukan ansambel.

Gagasan utama *boosting* adalah menambahkan model-model baru ke ansambel secara berurutan. Pada setiap iterasi tertentu, model pembelajaran dasar-lemah yang baru dilatih sehubungan dengan kesalahan seluruh rangkaian yang dipelajari sejauh ini.

![Pendekatan boosting](http://uc-r.github.io/public/images/analytics/gbm/boosted-trees-process.png)

Mari kita bahas masing-masing komponen kalimat sebelumnya dengan lebih detail karena mereka penting untuk diketahui.

**Base-learning models**: *Boosting* adalah kerangka kerja yang secara iteratif meningkatkan model pembelajaran yang lemah. Banyak aplikasi *gradient boosting* memungkinkan kita untuk "memasukkan" berbagai kelas *weak learner* sesuai keinginan kita Namun dalam praktiknya, algoritma yang ditingkatkan hampir selalu menggunakan *decision trees* sebagai *base learner*-nya. Konsekuensinya, tutorial ini akan membahas *boosting* dalam konteks pohon regresi atau klasifikasi.

**Training weak models**: *Weak model* adalah model yang tingkat kesalahannya hanya sedikit lebih baik daripada menebak secara acak. Gagasan di balik *boosting* adalah bahwa setiap model berurutan membangun model lemah sederhana untuk sedikit memperbaiki kesalahan yang tersisa. Sehubungan dengan *decision trees*, pohon dangkal mewakili *weak learner*. Umumnya, pohon dengan hanya 1-6 pohon digunakan. Menggabungkan banyak *weak model* (versus yang kuat) memiliki beberapa manfaat:

* *Speed*: Membangun model lemah adalah murah secara proses komputasi
* *Accuracy improvement*: *Weak model* memungkinkan algoritma untuk belajar secara lambat; melakukan penyesuaian kecil di area baru yang kinerjanya tidak baik. Secara umum, pendekatan statistik *weak learner* cenderung berkinerja baik.
* *Avoids overfitting*: Karena hanya membuat perbaikan bertahap kecil dengan masing-masing model dalam ansambel, ini memungkinkan kita untuk menghentikan proses pembelajaran segera setelah overfitting telah terdeteksi (biasanya dengan menggunakan validasi silang).

**Sequential training with respect to errors**: *Boosted trees* ditumbuhkan secara berurutan; setiap pohon ditumbuhkan menggunakan informasi dari pohon yang sebelumnya ditumbuhkan. Algoritma dasar untuk *boosted model* dapat digeneralisasi ke yang berikut ini di mana $x$ mewakili fitur data dan variabel $y$ mewakili respons:

1. Buat *decision trees* pada data: $F_1\left(x\right)=y$
2. Buat *decision trees* selanjutnya menggunakan data residual dari *decision trees* sebelumnya: $h_1\left(x\right)=y-F_1\left(x\right)$.
3. Tambahkan pohon baru tersebut ke dalam algoritma: $F_2\left(x\right)= F_1\left(x\right)+h_1\left(x\right)$
4. Buat *decision trees* baru pada residu $F_2$: $h_2\left(x\right)=y - F_2\left(x\right)$
5. Tambahkan *decision trees* tersebut ke dalam algoritma: $F_3\left(x\right)=F_2\left(x\right)+h_2\left(x\right)$
6. Lanjutkan proses tersebut hingga sebuah mekanisme (biasanya hasil validasi silang) menyatakan proses tersebut harus berhenti.

## Gradient Descent

Banyak algoritma, termasuk pohon keputusan, fokus pada meminimalkan residu dan oleh karena itu, menekankan fungsi *loss* MSE. Algoritma yang dibahas pada bagian sebelumnya menguraikan pendekatan *sequantial decision trees fitting* untuk meminimalkan kesalahan. Pendekatan khusus ini adalah bagaimana meningkatkan gradien meminimalkan fungsi *loss* *mean squared error* (MSE). Namun, seringkali kita ingin fokus pada fungsi *loss* lainnya seperti *mean absolute error* (MAE) atau untuk dapat menerapkan metode ini ke masalah klasifikasi dengan fungsi *loss* seperti *deviance*. *Gradient boosting machine*  berasal dari fakta bahwa prosedur ini dapat digeneralisasi ke fungsi *loss* selain MSE.

*Gradient Boosting* dianggap sebagai algoritma *gradient descent*. *Gradient descent* adalah algoritma optimasi yang sangat umum yang mampu menemukan solusi optimal untuk berbagai masalah. Gagasan umum *gradient descent* adalah mengubah parameter secara iteratif untuk meminimalkan fungsi *cost*. Misalkan kita adalah pemain ski menuruni bukit dan berpacu dengan teman kita. Strategi yang baik untuk mengalahkan teman kita ke bawah adalah mengambil jalan setapak dengan kemiringan paling curam. Inilah yang dilakukan oleh *gradient descent* - ini mengukur gradient lokal dari fungsi *loss* (*cost*) untuk sekumpulan parameter ($\Theta$) dan mengambil langkah-langkah ke arah gradien yang menurun. Setelah gradien nol, kita telah mencapai minimum.


![Gradient descent (Sumber: Geron, 2017)](http://uc-r.github.io/public/images/analytics/gbm/gradient_descent.png)

Gradient descent dapat dilakukan pada setiap fungsi *loss* yang dapat diturunkan (*differentiable*).  Akibatnya, ini memungkinkan GBM untuk mengoptimalkan berbagai fungsi *loss* seperti yang diinginkan. Parameter penting dalam *gradient descent* adalah *step size* yang ditentukan oleh *learning rate*. Jika *learning rate* terlalu kecil, maka algoritma akan mengambil banyak iterasi untuk menemukan minimum. Di sisi lain, jika tingkat pembelajaran terlalu tinggi, kita mungkin melewati batas minimum dan berakhir lebih jauh daripada saat kita mulai.

![Perbandinga learning rate (Sumber: Geron, 2017)](http://uc-r.github.io/public/images/analytics/gbm/learning_rate_comparison.png)

Selain itu, tidak semua fungsi *cost* bersifat *covex* (berbentuk mangkuk). Mungkin ada *local minimas*, *plateaus*, dan medan tidak teratur lainnya dari fungsi *loss* yang membuat sulit menemukan minimum global. *Stochastic gradient descent* dapat membantu kita mengatasi masalah ini dengan mengambil sampel sebagian kecil dari pengamatan pelatihan (biasanya tanpa penggantian) dan menumbuhkan pohon berikutnya menggunakan subsampel itu. Ini membuat algoritma lebih cepat tetapi sifat stokastik dari random sampling juga menambahkan beberapa sifat acak dalam menuruni gradien fungsi *loss*. Meskipun keacakan ini tidak memungkinkan algoritma untuk menemukan minimum global absolut, itu sebenarnya dapat membantu algoritma melompat keluar dari minimum lokal dan mematikan *plateus* dan mendekati minimum global.

![Stocahstic gradient descent (Sumber: Geron, 2017)](http://uc-r.github.io/public/images/analytics/gbm/stochastic_gradient_descent.png)

## Kelebihan dan Kekurangan

**Kelebihan**

* Seringkali memberikan akurasi prediksi yang tidak dapat dikalahkan.
* Banyak fleksibilitas - dapat mengoptimalkan berbagai fungsi *loss* dan menyediakan beberapa opsi *hyperparameter tuning* yang membuat fungsi ini sangat fleksibel.
* Tidak diperlukan pra-pemrosesan data - seringkali berfungsi dengan baik dengan nilai kategorikal dan numerik sebagaimana adanya.
* Menangani data yang hilang - tidak diperlukan imputasi.

**Kekurangan**

* GBM akan terus ditingkatkan untuk meminimalkan semua kesalahan. Ini bisa terlalu menekankan outlier dan menyebabkan overfitting. Harus menggunakan validasi silang untuk menetralisir.
* Mahal secara komputasi - GBM sering membutuhkan banyak pohon (> 1000) yang bisa menghabiskan banyak waktu dan memori.
* Fleksibilitas yang tinggi menghasilkan banyak parameter yang berinteraksi dan sangat memengaruhi perilaku pendekatan (jumlah iterasi, kedalaman pohon, parameter regularisasi, dll.). Ini membutuhkan pencarian kotak besar selama penyetelan.
* Kurang dapat diartikan meskipun hal ini mudah diatasi dengan berbagai alat (variable importance, partial dependence plots, LIME, dll.).

## Validasi Silang dan Parameter Tuning

Lngkah pertama yang perlu dilakukan untuk *training* GBM adalah menyetel parameter validasi silang dan *hyperparameter tuning*. Pengaturan kedua hal tersebut ditampilkan pada sintaks berikut:

```{r boost-cv, cache=TRUE}
# spesifikasi metode validasi silang
cv <-trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 5, 
  # repeats = 5,
  classProbs = TRUE, 
  sampling = "smote",
  summaryFunction = multiClassSummary,
  savePredictions = TRUE,
  allowParallel = TRUE
)
```

```{r boost-grid, cache=TRUE}
hyper_grid <- expand.grid(
  n.trees = 1000,
  shrinkage = 0.01,
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 10, 15)
)
```

Proses training dilakukan dengen menyetel argumen `method` sebagai `gbm` yang mendandakan kita akan melakukan *training* GBM.

```{r boost-fit, cache=TRUE}
system.time(
model_fit_cv <- train(
  blueprint,
  data = data_train,
  method = "gbm",
  trControl = cv,
  tuneGrid = hyper_grid,
  verbose = FALSE,
  metric = "AUC"
  )
)

model_fit_cv
```

Proses *training* berlangsung selama 3333.327 detik. Terdapat 9 buah model yang dibentuk, dimana model dengan **RMSE** terkecil memiliki nilai *hyperparameter* `shrinkage = 0,01`, `n.trees = 6000`, `interaction.depth = 7` dan `n.minobsinnode = 15`. Nilai **RMSE** model tersebut adalah sebagai berikut:

```{r boost-rmse, cache=TRUE}
rmse <- model_fit_cv$results %>%
  arrange(-AUC) %>%
  slice(1) %>%
  select(AUC) %>% pull()

rmse
```

Nilai **RMSE** rata-rata yang diperoleh sedikit lebih baik dibandingkan model *random forest*.

Visualisasi nilai *hyperparameter* terhadap **RMSE** model ditampilkan sebagai berikut:

```{r boost-vis, cache=TRUE}
ggplot(model_fit_cv)
```


## Model Akhir

Model terbaik dapat diekstrak dengan sintaks berikut:

```{r boost-final, cache=TRUE}
model_fit <- model_fit_cv$finalModel
```


Performa model dalam memprediksi data baru dapat dilihat berdasarkan **RMSE* pada data *test*.

```{r boost-test-rmse, cache=TRUE}
pred_test <- predict(model_fit, n.trees = model_fit$n.trees,
                     baked_test, type = "response")
pred_test <-
  as.data.frame(pred_test) %>%
  rowid_to_column("row") %>%
  pivot_longer(cols = matches("Jas|Mar|Quee"), names_to = "artist", values_to = "prob") %>%
  mutate(artist = str_remove_all(artist, ".1000")) %>%
  dplyr::group_by(row) %>%
  slice(which.max(prob)) %>%
  mutate(artist = factor(artist, 
                             labels = c("Jason_Mraz", "Maroon_5", "Queen" ))) %>%
  dplyr::select(artist) %>%
  pull()

## confusion matrix
confusionMatrix(pred_test, baked_test$artist)
```

## Interpretasi Fitur

Untuk melihat fitur yang paling berpengaruh dalam model, kita dapat menggunakan *variable importance plot*.

```{r boost-vip, cache=TRUE}
vi <- vip(model_fit_cv, num_features = 10)
vi
```





















