---
title: "Bagging"
author: "Moh. Rosidi"
date: "7/22/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Dataset Spotify

Pada artikel ini, kita akan membuat model prediktif pada dataset `Spotify`. `Spotify`  Merupakan dataset yang berisikan daftar lagu dan fitur audio dari band/penyanyi ternama dunia, antara lain: Queens, Maroon 5, dan Jason Mraz.

Kolom-kolom pada dataset tersebut, antara lain:

* `id` : nomor seri lagu
*  `name` : judul lagu
* `popularity` : tingkat popularitas lagu
* `album.id` : nomor seri album
* `album.name` : nama album
* `album.total_tracks` : total lagu dalam album
* `track_number` : nomor lagu dalam album
* `duration_ms` : durasi lagu dalam satuan ms
* `danceability` : elemen musik termasuk tempo, stabilitas ritme, kekuatan beat, dan keteraturan keseluruhan. Nilai 0,0 paling tidak bisa digunakan untuk menari dan 1,0 paling bisa digunakan untuk menari.
* `energy` : Energi adalah ukuran dari 0,0 hingga 1,0 dan mewakili ukuran persepsi intensitas dan aktivitas. Biasanya, trek yang energik terasa cepat, keras, dan berisik. Sebagai contoh, death metal memiliki energi tinggi, sedangkan prelude Bach mendapat skor rendah pada skala. Fitur perseptual yang berkontribusi pada atribut ini meliputi rentang dinamis, persepsi kenyaringan, warna nada, onset rate, dan entropi umum.
* `key` : Kunci dari trek adalah. Integer memetakan ke pitch menggunakan notasi Pitch Class standar. Misalnya. 0 = C, 1 = C♯ / D ♭, 2 = D, dan seterusnya.
* `loudness` : Keseluruhan kenyaringan trek dalam desibel (dB). Nilai kenyaringan rata-rata di seluruh trek dan berguna untuk membandingkan kenyaringan relatif trek. Kenyaringan adalah kualitas suara yang merupakan korelasi psikologis utama dari kekuatan fisik (amplitudo). Nilai kisaran khas antara -60 dan 0 db.
* `mode` : Mode menunjukkan modalitas (besar atau kecil) dari suatu trek, jenis skala dari mana konten melodinya diturunkan. Mayor diwakili oleh 1 dan minor adalah 0.
* `speechiness` : Speechiness mendeteksi keberadaan kata-kata yang diucapkan di trek. Semakin eksklusif pidato-seperti rekaman (mis. Acara bincang-bincang, buku audio, puisi), semakin dekat dengan 1.0 nilai atribut. Nilai di atas 0,66 menggambarkan trek yang mungkin seluruhnya terbuat dari kata-kata yang diucapkan. Nilai antara 0,33 dan 0,66 menggambarkan trek yang mungkin berisi musik dan ucapan, baik dalam bagian atau lapisan, termasuk kasus-kasus seperti musik rap. Nilai di bawah 0,33 kemungkinan besar mewakili musik dan trek non-ucapan lainnya.
* `acousticness` : Ukuran kepercayaan dari 0,0 hingga 1,0 dari apakah trek akustik. 1.0 mewakili kepercayaan tinggi trek adalah akustik.
* `instrumentalness` : Memprediksi apakah suatu lagu tidak mengandung vokal. Suara “Ooh” dan “aah” diperlakukan sebagai instrumen dalam konteks ini. Rap atau trek kata yang diucapkan jelas "vokal". Semakin dekat nilai instrumentalness ke 1.0, semakin besar kemungkinan trek tidak mengandung konten vokal. Nilai di atas 0,5 dimaksudkan untuk mewakili trek instrumental, tetapi kepercayaan diri lebih tinggi ketika nilai mendekati 1.0.
* `liveness` : Mendeteksi keberadaan audiens dalam rekaman. Nilai liveness yang lebih tinggi mewakili probabilitas yang meningkat bahwa trek dilakukan secara langsung. Nilai di atas 0,8 memberikan kemungkinan kuat bahwa trek live.
* `valence` : Ukuran 0,0 hingga 1,0 yang menggambarkan kepositifan musik yang disampaikan oleh sebuah trek. Lagu dengan valensi tinggi terdengar lebih positif (mis. Bahagia, ceria, gembira), sedangkan trek dengan valensi rendah terdengar lebih negatif (mis. Sedih, tertekan, marah).
* `tempo` : Perkiraan tempo trek secara keseluruhan dalam beat per menit (BPM). Dalam terminologi musik, tempo adalah kecepatan atau kecepatan dari bagian yang diberikan dan diturunkan langsung dari durasi beat rata-rata.
* `time_signature` : An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).

# Persiapan {.tabset}

## Library

Terdapat beberapa paket yang digunakan dalam pembuatan model prediktif menggunakan *bagging*. Paket-paket yang digunakan ditampilkan sebagai berikut:

```{r import-lib, cache=TRUE}
# library pembantu
library(e1071)
library(rsample)
library(recipes)
library(DataExplorer)
library(skimr)
library(DMwR)
library(MLmetrics)
library(tidyverse)

# library model
library(caret) 
library(ipred)


# paket penjelasan model
library(rpart.plot)  
library(vip)
library(pdp)
```

**Paket Pembantu**

1.  `e1071` : paket dengan sejumlah fungsi untuk melakukan *latent class analysis, short time Fourier transform, fuzzy clustering, support vector machines, shortest path computation, bagged clustering, naive Bayes classifier*, dll. Paket ini merupakan paket pembantu dalam proses *fitting* sejumlah model pohon
2. `tidyverse` : kumpulan paket dalam bidang data science
3. `rsample` : membantu proses *data splitting*
4. `recipes`: membantu proses data pra-pemrosesan
5. `DataExplorer` : EDA
6. `skimr` : membuat ringkasan data
7. `DMwR` : paket untuk melakukan sampling "smote"

**Paket untuk Membangun Model**

1. `caret` : berisikan sejumlah fungsi yang dapat merampingkan proses pembuatan model regresi dan klasifikasi
2. `ipred` : membentuk model *bagging*


**Paket Interpretasi Model**

1. `rpart.plot` : visualisasi *decision trees*
2. `vip` : visualisasi *variable importance*
3. `pdp` : visualisasi plot ketergantungan parsial

## Import Dataset

Import dataset dilakukan dengan menggunakan fungsi `readr()`. Fungsi ini digunakan untuk membaca file dengan ekstensi `.csv`.

```{r import-data, cache=TRUE}
spotify <- read_csv("data/spotify.csv")

# data cleaning
key_labs = c('c', 'c#', 'd', 'd#', 'e', 'f', 
             'f#', 'g', 'g#', 'a', 'a#', 'b')
mode_labs = c('minor', 'major')

spotify <- spotify %>%
  select(popularity, duration_ms:artist) %>%
  mutate(time_signature = factor(time_signature),
         key = factor(key, labels = key_labs),
         mode = factor(mode, labels = mode_labs),
         artist = factor(artist, labels = c("Jason_Mraz", "Maroon_5", "Queen" )))
```


# Data Splitting

Proses *data splitting* dilakukan setelah data di import ke dalam sistem. Hal ini dilakukan untuk memastikan tidak adanya kebocoran data yang mempengaruhi proses pembuatan model. Data dipisah menjadi dua buah set, yaitu: *training* dan *test*. Data *training* adalah data yang akan kita gunakan untuk membentuk model. Seluruh proses sebelum uji model akan menggunakan data *training*. Proses tersebut, antara lain: EDA, *feature engineering*, dan validasi silang. Data *test* hanya digunakan saat kita akan menguji performa model dengan data baru yang belum pernah dilihat sebelumnya.

Terdapat dua buah jenis sampling pada tahapan *data splitting*, yaitu:

1. *random sampling* : sampling acak tanpa mempertimbangkan adanya strata dalam data
2. *startified random sampling* : sampling dengan memperhatikan strata dalam sebuah variabel.

Dalam proses pembentukan model kali ini, kita akan menggunakan metode kedua dengan tujuan untuk memperoleh distribusi yang seragam dari variabel target (`artist`).

```{r data-split, cache=TRUE}
set.seed(123)

split  <- initial_split(spotify, prop = 0.8, strata = "artist")
data_train  <- training(split)
data_test   <- testing(split)
```

Untuk mengecek distribusi dari kedua set data, kita dapat mevisualisasikan distribusi dari variabel target pada kedua set tersebut.

```{r target-vis, cache=TRUE}
# training set
ggplot(data_train, aes(x = artist)) + 
  geom_bar() 
# test set
ggplot(data_test, aes(x = artist)) + 
  geom_bar() 
```


# Analisis Data Eksploratif

Analsiis data eksploratif (EDA) ditujukan untuk mengenali data sebelum kita menentukan algoritma yang cocok digunakan untuk menganalisa data lebih lanjut. EDA merupakan sebuah proses iteratif yang secara garis besar menjawab beberapa pertanyaan umum, seperti:

1. Bagaimana distribusi data pada masing-masing variabel?
2. Apakah terdapat asosiasi atau hubungan antar variabel dalam data?

## Ringkasan Data

Terdapat dua buah fungsi yang digunakan dalam membuat ringkasan data, antara lain:

1. `glimpse()`: varian dari `str()` untuk mengecek struktur data. Fungsi ini menampilkan transpose dari tabel data dengan menambahkan informasi, seperti: jenis data dan dimensi tabel.
2. `skim()` : fungsi dari paket `skimr` untuk membuat ringkasan data yang lebih detail dibanding `glimpse()`, seperti: statistika deskriptif masing-masing kolom, dan informasi *missing value* dari masing-masing kolom.
3. `plot_missing()` : fungsi untuk memvisualisasikan persentase *missing value* pada masing-masing variabel atau kolom data


```{r glimpse, cache=TRUE}
glimpse(data_train)
```

```{r skim, cache=TRUE}
skim(data_train)
```

```{r missing-vis, cache=TRUE}
plot_missing(data_train)
```

Berdasarkan ringkasan data yang dihasilkan, diketahui dimensi data sebesar 982 baris dan 15 kolom. Dengan rincian masing-masing kolom, yaitu: 4 kolom dengan jenis data factor dan 11 kolom dengan jenis data numeric. Informasi lain yang diketahui adalah seluruh kolom dalam data tidak memiliki *missing value*.

## Variasi

Variasi dari tiap variabel dapat divisualisasikan dengan menggunakan histogram (numerik) dan baplot (kategorikal).

```{r hist, cache=TRUE}
plot_histogram(data_train, ncol = 2L, nrow = 2L)
```

```{r bar, cache=TRUE}
plot_bar(data_train, ncol = 2L, nrow = 2L)
```

Berdasarkan hasil visualisasi diperoleh bahwa sebagian besar variabel numerik memiliki distribusi yang tidak simetris. Sedangkan pada variabel kategorikal diketahui bahwa seluruh variabel memiliki variasi yang tidak mendekati nol atau nol. Untuk mengetahui variabel dengan variasi mendekati nol atau nol, dapat menggunakan sintaks berikut:

```{r nzv, cache=TRUE}
nzvar <- nearZeroVar(data_train, saveMetrics = TRUE) %>% 
  rownames_to_column() %>% 
  filter(nzv)
nzvar
```

## Kovarian

Kovarian dapat dicek melalui visualisasi *heatmap* koefisien korelasi.

```{r heatmap, cache=TRUE}
plot_correlation(data_train, 
                 cor_args = list(method = "spearman"))
```

# Target and Feature Engineering

*Data preprocessing* dan *engineering* mengacu pada proses penambahan, penghapusan, atau transformasi data. Waktu yang diperlukan untuk memikirkan identifikasi kebutuhan *data engineering* dapat berlangsung cukup lama dan proprsinya akan menjadi yang terbesar dibandingkan analisa lainnya. Hal ini disebabkan karena kita perlu untuk memahami data apa yang akan kita oleh atau diinputkan ke dalam model.

Untuk menyederhanakan proses *feature engineerinh*, kita harus memikirkannya sebagai sebuah *blueprint* dibanding melakukan tiap tugasnya secara satu persatu. Hal ini membantu kita dalam dua hal:

1. Berpikir secara berurutan
2. Mengaplikasikannya secara tepat selama proses *resampling*

## Urutan Langkah-Langkah Feature Engineering

Memikirkan *feature engineering* sebagai sebuah *blueprint* memaksa kita untuk memikirkan urutan langkah-langkah *preprocessing* data. Meskipun setiap masalah mengharuskan kita untuk memikirkan efek *preprocessing* berurutan, ada beberapa saran umum yang harus kita pertimbangkan:

* Jika menggunakan log atau transformasi Box-Cox, jangan memusatkan data terlebih dahulu atau melakukan operasi apa pun yang dapat membuat data menjadi tidak positif. Atau, gunakan transformasi Yeo-Johnson sehingga kita tidak perlu khawatir tentang hal ini.
* *One-hot* atau *dummy encoding* biasanya menghasilkan data jarang (*sparse*) yang dapat digunakan oleh banyak algoritma secara efisien. Jika kita menstandarisasikan data tersebut, kita akan membuat data menjadi padat (*dense*) dan kita kehilangan efisiensi komputasi. Akibatnya, sering kali lebih disukai untuk standardisasi fitur numerik kita dan kemudian *one-hot/dummy endode*.
* Jika kila mengelompokkan kategori (*lumping*) yang jarang terjadi  secara bersamaan, lakukan sebelum *one-hot/dummy endode*.
* Meskipun kita dapat melakukan prosedur pengurangan dimensi pada fitur-fitur kategorikal, adalah umum untuk melakukannya terutama pada fitur numerik ketika melakukannya untuk tujuan rekayasa fitur.

Sementara kebutuhan proyek kita mungkin beragam, berikut ini adalah urutan langkah-langkah potensial yang disarankan untuk sebagian besar masalah:

1. Filter fitur dengan varians nol (*zero varians*) atau hampir nol (*near zero varians*).
2. Lakukan imputasi jika diperlukan.
3. Normalisasi untuk menyelesaikan *skewness* fitur numerik.
4. Standardisasi fitur numerik (*centering* dan *scaling*).
5. Lakukan reduksi dimensi (mis., PCA) pada fitur numerik.
6. *one-hot/dummy endode* pada fitur kategorikal.

## Meletakkan Seluruh Proses Secara Bersamaan

Untuk mengilustrasikan bagaimana proses ini bekerja bersama menggunakan R, mari kita lakukan penilaian ulang sederhana pada set data `ames` yang kita gunakan  dan lihat apakah beberapa *feature engineering* sederhana meningkatkan kemampuan prediksi model kita. Tapi pertama-tama, kita berkenalan dengat paket `recipe`.

Paket `recipe` ini memungkinkan kita untuk mengembangkan *bluprint feature engineering* secara berurutan. Gagasan di balik `recipe` mirip dengan `caret :: preProcess()` di mana kita ingin membuat *blueprint preprocessing* tetapi menerapkannya nanti dan dalam setiap resample.

Ada tiga langkah utama dalam membuat dan menerapkan rekayasa fitur dengan `recipe`:

1. `recipe()`: tempat kita menentukan langkah-langkah rekayasa fitur untuk membuat *blueprint*.
2. `prep()`: memperkirakan parameter *feature engineering* berdasarkan data *training*.
3. `bake()`: terapkan *blueprint* untuk data baru.

```{r}
blueprint <- recipe(artist ~ ., data = data_train) %>%
  step_nzv(all_nominal())  %>%
  
  # 2. imputation to missing value
  # step_medianimpute("<Num_Var_name>") %>% # median imputation
  # step_meanimpute("<Num_var_name>") %>% # mean imputation
  # step_modeimpute("<Cat_var_name>") %>% # mode imputation
  # step_bagimpute("<Var_name>") %>% # random forest imputation
  # step_knnimpute("<Var_name>") %>% # knn imputation
  
  # Label encoding for categorical variable with many classes 
  # step_integer("<Cat_var_name>") %>%
  
  # 3. normalize to resolve numeric feature skewness
  step_center(all_numeric(), -all_outcomes()) %>%
  
  # 4. standardize (center and scale) numeric feature
  step_scale(all_numeric(), -all_outcomes()) 
```

Selanjutnya, *blueprint* yang telah dibuat dilakukan *training* pada data *training*. Perlu diperhatikan, kita tidak melakukan proses *training* pada data *test* untuk mencegah *data leakage*.

```{r prep, cache=TRUE}
prepare <- prep(blueprint, training = data_train)
prepare
```

Langkah terakhir adalah mengaplikasikan *blueprint* pada data *training* dan *test* menggunakan fungsi `bake()`.

```{r baked, cache=TRUE}
baked_train <- bake(prepare, new_data = data_train)
baked_test <- bake(prepare, new_data = data_test)
baked_train
```

# Bagging

Seperti disebutkan sebelumnya, model pohon tunggal memiliki kekurangan, yaitu: varian yang tinggi. Hal ini berarti algoritma *decision tress* memodelkan *noise* dalam data *training*-nya. Meskipun pemangkasan pohon yang terbentuk membantu mengurangi varians ini, ada metode alternatif yang sebenarnya mengeksploitasi variabilitas pohon tunggal dengan cara yang secara signifikan dapat meningkatkan kinerja lebih dan di atas pohon tunggal. Agregat bootstrap (bagging) adalah salah satu pendekatan yang dapat digunakan (awalnya diusulkan oleh [Breiman, 1996](https://link.springer.com/article/10.1023%2FA%3A1018054314350)).

Bagging menggabungkan dan merata-rata beberapa model. Rata-rata di beberapa pohon mengurangi variabilitas dari satu pohon dan mengurangi overfitting dan meningkatkan kinerja prediksi. Bagging mengikuti tiga langkah sederhana:

1. Buat sampel [bootstrap](http://uc-r.github.io/bootstrapping) $m$ dari data *training*. Sampel bootstrap memungkinkan kita untuk membuat banyak set data yang sedikit berbeda tetapi dengan distribusi yang sama dengan set data *training* secara keseluruhan.
2. Untuk setiap sampel bootstrap, latih satu pohon regresi tanpa melakukan pemangkasan (*unpruned*)
3. Lakukan prediksi data *test* pada tiap pohon yang terbentuk dari setiap pohon. Hasil prediksi masing-masing pohon selanjutnya dirata-rata.

![Proses bagging (Sumber: <http://uc-r.github.io/>)](http://uc-r.github.io/public/images/analytics/regression_trees/bagging3.png)

Proses ini sebenarnya dapat diterapkan pada model regresi atau klasifikasi apa pun; Namun, metode ini memberikan peningkatan terbesar pada model yang memiliki varian tinggi. Sebagai contoh, model parametrik yang lebih stabil seperti regresi linier dan splines regresi multi-adaptif cenderung kurang mengalami peningkatan dalam kinerja prediksi.

Salah satu manfaat bagging adalah  rata-rata, sampel bootstrap akan berisi 63% (2/3) bagian dari data *training*. Ini menyisakan sekitar 33% (1/3) data dari sampel yang di-bootstrap. Kita dapat menyebutnya sebagai sampel *out-of-bag* (OOB). Kita dapat menggunakan pengamatan OOB untuk memperkirakan akurasi model, menciptakan proses validasi silang alami.

## Validasi Silang

Spesifikasi validasi silang yang digunakan untuk membuat model bagging sama dengan spesifikasi validasi silang model *decission tree*. Perbedaannya adalah pada argumen `trainControl()` tidak ditambahkan argumen `sample`. Hal ini disebabkan pada model bagging yang dibuat ini tidak dilakukan *parameter tuning*.

```{r bag-cv, cache=TRUE}
# spesifikasi metode validasi silang
cv <-trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10, 
  # repeats = 5,
  classProbs = TRUE, 
  search = "random",
  sampling = "smote",
  summaryFunction = multiClassSummary,
  savePredictions = TRUE,
  allowParallel = TRUE
)
```

Agumen `method` yang digunakan dalam model ini adalah `"treebag"` yang merupakan `method` yang digunakan jika model bagging yang akan dibuat menggunakan paket `ipred`.

```{r bag-fit, cache=TRUE}
system.time(
model_fit_cv <- train(
  blueprint,
  data = data_train,
  method = "treebag",
  trControl = cv,
  importance = TRUE,
  metric = "AUC"
  )
)

model_fit_cv
```


Proses *training* model berlangsung selama 18.209 detik dengan rata-rata **AUC** yang diperoleh sebesar 'r model_fit_cv$results %>% select(AUC) %>% pull()'. Nilai ini merupakan peningkatan dari model *decision trees* yang telah dibuat sebelumnya. 

```{r bag-roc, cache=TRUE}
roc <- model_fit_cv$results %>%
  arrange(-AUC) %>%
  slice(1) %>%
  select(AUC) %>% pull()

roc
```

## Model Akhir

Pada tahapan ini, model yang telah di-*training*, diekstrak model akhirnya.

```{r bag-final, cache=TRUE}
model_fit <- model_fit_cv$finalModel
```

Adapun performa model bagging pada data baru dapat dicek dengan mengukur nilai **Akurasi** model menggunakan data *test*.

```{r bag-rmse-test, cache=TRUE}
# prediksi Sale_Price churn_test
pred_test <- predict(model_fit, baked_test, type = "class")

## RMSE
cm <- confusionMatrix(pred_test, baked_test$artist)
cm
```

Berdasarkan hasil evaluasi diperoleh nilai akurasi sebesar `r cm$overall[1]`

## Interpretasi Fitur

Untuk melakukan interpretasi terhadap fitur paling berpengaruh dalam model bagging, kita dapat emngetahuinya melalui *varibale importance plot*.

```{r bag-vip, cache=TRUE}
vi <- vip(model_fit_cv, num_features = 10)
vi
```

Berdasarkan hasil plot, terdapat empat buah variabel paling berpengaruh, yaitu: `r vi$data %>% select(Variable) %>% pull() %>%.[1:4]`. Untuk melihat efek dari keempat variabel tersebut terhadap prediksi yang dihasilkan model, kita dapat mengetahuinya melalui *patial plot dependencies*.

```{r bag-pdp, cache=TRUE}
p1 <- pdp::partial(model_fit_cv, pred.var = vi$data %>% select(Variable) %>% pull() %>%.[1]) %>% 
  autoplot() 

p2 <- pdp::partial(model_fit_cv, pred.var = vi$data %>% select(Variable) %>% pull() %>%.[2]) %>% 
  autoplot()

p3 <- pdp::partial(model_fit_cv, pred.var = vi$data %>% select(Variable) %>% pull() %>%.[3]) %>% 
  autoplot()
  

p4 <- pdp::partial(model_fit_cv, pred.var = vi$data %>% select(Variable) %>% pull() %>%.[4]) %>% 
  autoplot()

grid.arrange(p1, p2, p3, p4, nrow = 2)
```


