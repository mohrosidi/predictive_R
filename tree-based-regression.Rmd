---
title: "Analisis Prediktif Harga Rumah Menggunakan Tree Based Algorith"
author: "Moh. Rosidi"
date: "7/20/2020"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
    df_print: paged
    theme: yeti
    highlight: textmate
    css: assets/style.css
  pdf_document:
    toc: yes
    toc_depth: '3'
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Pengantar



# Dataset Ames



# Persiapan {.tabset}

## Library

```{r import-lib, cache=TRUE}
# library pembantu
library(plyr)
library(e1071)
library(foreach)
library(import)
library(tidyverse)
library(rsample)
library(recipes)
library(DataExplorer)
library(skimr)
library(modeldata)

# library model
library(caret) 
library(rpart)
library(ipred)
library(randomForest)
library(gbm)

# paket penjelasan model
library(rpart.plot)  
library(vip)         
```


## Import Dataset

```{r import-data, cache=TRUE}
data("ames")
```


# Data Splitting

```{r data-split, cache=TRUE}
set.seed(123)

split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```


```{r target-vis, cache=TRUE}
# training set
ggplot(ames_train, aes(x = Sale_Price)) + 
  geom_density() 
# test set
ggplot(ames_test, aes(x = Sale_Price)) + 
  geom_density() 
```


# Analisis Data Eksploratif

## Ringkasan Data

```{r glimpse, cache=TRUE}
glimpse(ames_train)
```

```{r skim, cache=TRUE}
skim(ames_train)
```

```{r missing-vis, cache=TRUE}
plot_missing(ames_train)
```


## Variasi

```{r hist, cache=TRUE}
plot_histogram(ames_train, ncol = 2L, nrow = 2L)
```

```{r bar, cache=TRUE}
plot_bar(ames_train, ncol = 2L, nrow = 2L)
```

```{r nzv, cache=TRUE}
nzvar <- nearZeroVar(ames_train, saveMetrics = TRUE) %>% 
  rownames_to_column() %>% 
  filter(nzv)
nzvar
```

```{r wt-nzv, cache=TRUE}
without_nzvar <- select(ames_train, !nzvar$rowname)
skim(without_nzvar)
```

```{r count-nominal, cache=TRUE}
# MS_SubClass 
count(ames_train, MS_SubClass) %>% arrange(n)
# Neighborhood
count(ames_train, Neighborhood) %>% arrange(n)
# Neighborhood
count(ames_train, Exterior_1st) %>% arrange(n)
# Exterior_2nd
count(ames_train, Exterior_2nd) %>% arrange(n)
```

```{r, cache=TRUE}
plot_bar(without_nzvar)
```


## Kovarian

```{r heatmap, cache=TRUE}
plot_correlation(ames_train, type = "continuous", 
                 cor_args = list(method = "spearman"))
```


# Target and Feature Engineering

```{r preprocess, cache=TRUE}
blueprint <- recipe(Sale_Price ~., data = ames_train) %>%
  # feature filtering
  step_nzv(all_nominal()) %>%
  # label encoding
  step_integer(dplyr::matches("Exterior|Neighbor|Sub|Qual|Cond|QC|Qu|Type")) %>%
  # standardization
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())
blueprint
```

```{r prep, cache=TRUE}
prepare <- prep(blueprint, training = ames_train)
prepare
```

```{r baked, cache=TRUE}
baked_train <- bake(prepare, new_data = ames_train)
baked_test <- bake(prepare, new_data = ames_test)
baked_train
```

```{r, cache=TRUE}
skim(baked_train)
```


# Decision Tree Model

*Tree-based models* adalah kelas algoritma nonparametrik yang bekerja dengan mempartisi ruang fitur ke sejumlah daerah yang lebih kecil (tidak tumpang tindih) dengan nilai respons yang sama menggunakan seperangkat *aturan pemisahan*. Prediksi diperoleh dengan memasang model yang lebih sederhana (misal: Konstanta seperti nilai respons rata-rata) di setiap wilayah. Metode membagi dan menaklukkan seperti itu dapat menghasilkan aturan sederhana yang mudah ditafsirkan dan divisualisasikan dengan diagram pohon. 

Ada banyak metode yang dapat digunakan membangun pohon regresi, tetapi salah satu yang tertua dikenal sebagai pendekatan pohon klasifikasi dan regresi (CART) yang dikembangkan oleh [Breiman et al. (1984)](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418). Tutorial ini berfokus pada bagian regresi CART. Pohon regresi dasar mempartisi data yang ditetapkan ke dalam subkelompok yang lebih kecil dan kemudian melakukan fitting konstanta sederhana untuk setiap pengamatan dalam subkelompok. Partisi dicapai dengan partisi biner berturut-turut (alias partisi rekursif) berdasarkan pada berbagai prediktor. Konstanta untuk memprediksi didasarkan pada nilai respons rata-rata untuk semua pengamatan yang termasuk dalam subkelompok tersebut.

Sebagai contoh, misalkan kita ingin memprediksi mil per galon mobil rata-rata berdasarkan ukuran silinder (`cyl`) dan tenaga kuda (`hp`). Semua pengamatan melalui pohon ini, dinilai pada simpul tertentu, dan lanjutkan ke kiri jika jawabannya "ya" atau lanjutkan ke kanan jika jawabannya "tidak". Jadi, pertama, semua pengamatan yang memiliki 6 atau 8 silinder pergi ke cabang kiri, semua pengamatan lainnya dilanjutkan ke cabang kanan. Selanjutnya, cabang kiri selanjutnya dipartisi oleh tenaga kuda. Pengamatan 6 atau 8 silinder dengan tenaga kuda yang sama atau lebih besar dari 192 dilanjutkan ke cabang kiri; mereka yang kurang dari 192 hp melanjutkan ke kanan. Cabang-cabang ini mengarah ke *terminal node* atau *leaf nodes* yang berisi nilai respons prediksi kita. Pada dasarnya, semua pengamatan (mobil dalam contoh ini) yang tidak memiliki 6 atau 8 silinder (cabang paling kanan) rata-rata 27 mpg. Semua pengamatan yang memiliki 6 atau 8 silinder dan memiliki lebih dari 192 hp (cabang paling kiri) rata-rata 13 mpg.

![Prediksi mpg berdasarkan variabel cyl dan hp (Sumber: <http://uc-r.github.io/>)](http://uc-r.github.io/public/images/analytics/regression_trees/ex_regression_tree.png)

Contoh sederhana tersebut dapat kita generalisasikan. Variabel respon kontinu $Y$ dan dua buah variabel input $X_1$ dan $X_2$. Partisi rekursif menghasilkan tiga buah area (*nodes*), yaitu: $R_1$, $R_2$, dan $R_3$ dimana model memprediksi $Y$ dengan sebuah konstanta $c_m$ pada area $R_m$:

$$
\hat{f}\left(X\right) = \sum_{m=1}^{3} c_{m} I\left(X_1,X_2\right) \in R_m
$$

## Menentukan Split pada Decision Tree

Pertama, penting untuk mewujudkan partisi variabel yang dilakukan secara *top-down*. Ini hanya berarti bahwa partisi yang dilakukan sebelumnya pada pohon yang terbentuk tidak akan berubah oleh partisi selanjutnya. Tetapi bagaimana partisi ini dibuat? Model dimulai dengan seluruh data,$S$, dan mencari setiap nilai berbeda dari setiap variabel input untuk menemukan prediktor dan nilai *split* yang membagi data menjadi dua area ($R_1$ dan $R_2$) sedemikian rupa sehingga jumlah kesalahan kuadrat keseluruhan diminimalkan:

$$
minimize \left{SSE = \sum_{i \in R_1}^{ } \left(y_i-c_1\right)^2 +  \sum_{i \in R_2}^{ } \left(y_i-c_2\right)^2\right}
$$

Setelah menemukan  *split* terbaik, kita mempartisi data menjadi dua area yang dihasilkan dan mengulangi proses *split* pada masing-masing dua area Proses ini berlanjut sampai kriteria penghentian tercapai. Pohon yang dihasilkan biasanya sangat dalam, kompleks yang dapat menghasilkan prediksi yang baik pada data *training*, tetapi kemungkinan besar model yang dibuat *overfiting* dan akan menghasilkan hasil prediksi yang buruk pada data *test*.

## Cost complexity criterion

Seringkali ada keseimbangan yang harus dicapai dalam kedalaman dan kompleksitas pohon untuk mengoptimalkan kinerja prediksi pada beberapa data yang tidak terlihat. Untuk menemukan keseimbangan ini, kita biasanya menumbuhkan pohon yang sangat besar seperti yang didefinisikan pada bagian sebelumnya dan kemudian memangkasnya kembali untuk menemukan sub-pohon yang optimal. Kita menemukan sub-pohon optimal dengan menggunakan parameter kompleksitas biaya ($\alpha$) yang memberikan penalti pada fungsi objektif pada persamaan penentuan *split* untuk setiap *terminal nodes* pada tiap pohon ($T$).

$$
minimize\left{SSE+\alpha\left|T\right|\right}
$$

Untuk nilai $alpha$ yang diberikan, kita dapat menemukan pohon pemangkasan terkecil yang memiliki kesalahan penalti terendah. Jika kita terbiasa dengan regresi dengan penalti, kita akan menyadari hubungan dekat dengan penalti norma lasso $L_1$. Seperti dengan metode regularisasi ini, penalti yang lebih kecil cenderung menghasilkan model yang lebih kompleks dan menghasilkan pohon yang lebih besar. Sedangkan penalti yang lebih besar menghasilkan pohon yang jauh lebih kecil. Akibatnya, ketika pohon tumbuh lebih besar, pengurangan SSE harus lebih besar daripada penalti kompleksitas biaya. Biasanya, kita mengevaluasi beberapa model melintasi spektrum $\alpha$ dan menggunakan teknik validasi silang untuk mengidentifikasi $\alpha$ optimal dan sub-pohon optimal.

## Kelebihan dan Kekurangan

Terdapat sejumlah kelebihan penggunaan *decision trees*, antara lain:

* Mudah ditafsirkan.
* Dapat membuat prediksi cepat (tidak ada perhitungan rumit, hanya mencari konstanta di pohon).
* Sangat mudah untuk memahami variabel apa yang penting dalam membuat prediksi. Node internal (splits) adalah variabel-variabel yang sebagian besar mereduksi SSE.
* Jika ada beberapa data yang hilang, kita mungkin tidak bisa pergi jauh-jauh ke bawah pohon menuju daun, tetapi kita masih bisa membuat prediksi dengan merata-rata semua daun di sub-pohon yang kita jangkau.
* Model ini memberikan respons “bergerigi” non-linier, sehingga dapat bekerja saat permukaan regresi yang sebenarnya tidak mulus. Jika halus, permukaan konstan-piecewise dapat memperkirakannya secara dekat (dengan cukup daun).
* Ada algoritma yang cepat dan andal untuk mempelajari pohon-pohon ini.

Selain kelebihan, terdapat kekurangan dalam penggunaan *decision trees*, antara lain:

* Pohon regresi tunggal memiliki varian yang tinggi, menghasilkan prediksi yang tidak stabil (subsampel alternatif dari data *training* dapat secara signifikan mengubah node terminal).
* Karena varians tinggi pohon regresi tunggal memiliki akurasi prediksi yang buruk.

## Validasi Silang dan Parameter Tuning

Langkah pertama yang perlu dilakukan dalam melakukan kegiatan validasi silang adalah menentukan spesifikasi parameter validasi silang. Fungsi `trainControl()` merupakan fungsi yang dapat kita gunakan untu menetukan metode validasi silang yang dilakukan dan spesifikasi terkait metode validasi silang yang dugunakan.

Pada sintaks berikut dispesifikasikan `method` yang digunakan adalah `"cv"` dengan jumlah partisi sebanyak 10 buah. Parameter lain yang ikut ditambahkan dalam fungsi `trainControl()` adalah `search` yang menspesifikasikan metode *parameter tuning* yang dispesifikasikan dengan nilai `"random"`.

```{r, cache=TRUE}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10,
  search = "random",
  # repeats = 5,
  allowParallel = TRUE
)
```

Selanjutnya, hasil pengaturan parameter training diinputkan ke dalam fungsi `train()`. Dalam fungsi ini dispesifikasikan sejumlah argumen seperti: formula yang digunakan, data training yang akan digunakan, `method` atau `engine` yang akan digunakan untuk membentuk model. Proses *parameter tuning* diatur melalui argumen `tuneLength` yang merupakan kombinasi antar parameter yang akan di-*tuning* secara acak. Dalam hal ini dipsesifkasikan nilai `tuneLength` sebesar 10 yang menunjukkan 10 kombinasi *parameter-tuning* yang digunakan.

```{r, cache=TRUE}
system.time(
dt_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "rpart",
  trControl = cv,
  tuneLength = 10,
  metric = "RMSE"
  )
)

dt_fit_cv
```

Proses *training* berlangsung selama 53,94 detik dengan 8 buah model yang terbentuk. Model terbaik dipilih berdasarkan nilai **RMSE** terkecil. Berdasarkan kriteria tersebut model yang terpilih adalalah model yang memiliki nilai `cp = 0.0002333903` (cp : parameter kompleksitas atau penalti). Nilai **RMSE** rata-rata model terbaik adalah sebagai berikut:


```{r dt-rmse, cache=TRUE}
dt_rmse <- dt_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

dt_rmse
```

Visualisasi hubungan antara parameter kompleksitas dan **RMSE** ditampilkan pada gambar berikut:

```{r dt-cv-vis, chace = TRUE}
# visualisasi
ggplot(dt_fit_cv)
```


## Model Akhir

Model terbaik dari hasil proses validasi silang selanjutnya diekstrak. Hal ini berguna untuk mengurangi ukuran model yang tersimpan. Secara default fungsi `train()` akan mengembalikan model dengan performa terbaik. Namun, terdapat sejumlah komponen lain dalam objek yang terbentuk, seperti: hasil prediksi, ringkasan training, dll. yang membuat ukuran objek menjadi besar. Untuk menguranginya, kita perlu mengambil objek model final dari objek hasil validasi silang.

```{r dt-final, cache=TRUE}
dt_fit <- dt_fit_cv$finalModel
```

Visualisasi model final *decision tree*, dilakukan menggunakan fungsi `rpart.plot()`. 

```{r dt-vis, cache=TRUE}
# visualisasi
rpart.plot(dt_fit)
```

Besarnya dimensi (jumlah variabel) pada data menyebabkan hasil visualisasi model *decision tree* menjadi terlalu padat dan sulit menampilkan informasi pada tiap *node*-nya.

Cara lain untuk melihat performa sebuah model regresi adalah dengan melihat visualisasi nilai residunya. Berikut adalah sintaks yang digunakan:

```{r dt-res-vis, cache=TRUE}
pred_train <- predict(dt_fit, baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sp <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point()
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sp, hs, ncol = 2)
```

Berdasarkan hasil visualisasi dapat terlihat pola heteroskedastisitas pada *scatterplot* residu vs nilai aktual `Sale_Price`. Pola yang dihasilkan menunjukkan bahwa model dapat memprediksi dengan cukup baik pada rentang `Sale_Price` < 400.000. Sedangkan pada nilai diatasnya residu menghasilkan variasi yang cenderung terus melebar yang menandakan pada area ini model tidak menghasilkan prediksi yang cukup baik.

Model yang dihasilkan selanjutnya dapat kita uji lagi menggunakan data baru. Berikut adalah perhitungan nilai **RMSE** model pada data *test*.

```{r dt-rmse-test, cache=TRUE}
# prediksi Sale_Price ames_test
pred_test <- predict(dt_fit, baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```

## Interpretasi Fitur

Untuk mengetahui variabel yang paling berpengaruh secara global terhadap hasil prediksi model *decision tree*, kita dapat menggunakan plot *variable importance*.

```{r dt-vip, cache=TRUE}
vip(dt_fit_cv, num_features = 10)
```

Berdasarkan terdapat 2 buah variabel yang berpengaruh besar terhadap prediksi yang dihasilkan oleh model, antara lain: `Gr_Liv_Area`dan `Year_Remod_Add`. Untuk melihat efek dari masing-masing variabel terhadap variabel respon, kita dapat menggunakan *partial dependence plot*.

```{r dt-pdp, cache=TRUE}
p1 <- partial(dt_fit_cv, pred.var = "Gr_Liv_Area") %>% autoplot()
p2 <- partial(dt_fit_cv, pred.var = "Year_Remod_Add") %>% autoplot()
p3 <- partial(dt_fit_cv, pred.var = c("Gr_Liv_Area", "Year_Remod_Add")) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
              colorkey = TRUE, screen = list(z = -20, x = -60))


gridExtra::grid.arrange(p1, p2, p3, 
                        ncol=2)
```

Berdasarkan hasil visualisasi, terdapat relasi non-linear pada variabel variabel terhadap variabel target. Sebagai contoh variabel `Gr_liv_Area` memiliki relasi nonlinear terhadap `Sale_Price` yang ditunjukkan pada peningkatan nilai `Sale_Price` pada kisaran nilai  `Gr_liv_Area` 500-2400 dan kurva melandai dan cenderung konstan pada nilai `Gr_liv_Area` di atas 2400. Hal yang sama terjadi pula terhadap variabel `Year_Remod_Add`, dimana terjadi kenaikan tinggin pada tahun 1978-1979 dan melandai pada tahun-tahun setelahnya. Pada kurva interaksi, terlihat bahwa *decision trees* memiliki kurva prediksi rigid yang tidak halus. Hal ini sesuai dengan statemen yang telah dijelaskan di awal bahwa *decision trees* membagi data ke dalam sejumlah area dengan meminimisasi *impuritas* dalam pembagiannya. Pada kurva relasi juga dapat dilihat bahwa kenaikan nilai kedua fitur tersebut mempengaruhi kenaikan nilai variabel `Sale_Price`.

# Bagging

Seperti disebutkan sebelumnya, model pohon tunggal memiliki kekurangan, yaitu: varian yang tinggi. Hal ini berarti algoritma *decision tress* memodelkan *noise* dalam data *training*-nya. Meskipun pemangkasan pohon yang terbentuk membantu mengurangi varians ini, ada metode alternatif yang sebenarnya mengeksploitasi variabilitas pohon tunggal dengan cara yang secara signifikan dapat meningkatkan kinerja lebih dan di atas pohon tunggal. Agregat bootstrap (bagging) adalah salah satu pendekatan yang dapat digunakan (awalnya diusulkan oleh [Breiman, 1996](https://link.springer.com/article/10.1023%2FA%3A1018054314350)).

Bagging menggabungkan dan merata-rata beberapa model. Rata-rata di beberapa pohon mengurangi variabilitas dari satu pohon dan mengurangi overfitting dan meningkatkan kinerja prediksi. Bagging mengikuti tiga langkah sederhana:

1. Buat sampel [bootstrap](http://uc-r.github.io/bootstrapping) $m$ dari data *training*. Sampel bootstrap memungkinkan kita untuk membuat banyak set data yang sedikit berbeda tetapi dengan distribusi yang sama dengan set data *training* secara keseluruhan.
2. Untuk setiap sampel bootstrap, latih satu pohon regresi tanpa melakukan pemangkasan (*unpruned*)
3. Lakukan prediksi data *test* pada tiap pohon yang terbentuk dari setiap pohon. Hasil prediksi masing-masing pohon selanjutnya dirata-rata.

![Proses bagging (Sumber: <http://uc-r.github.io/>)](http://uc-r.github.io/public/images/analytics/regression_trees/bagging3.png)

Proses ini sebenarnya dapat diterapkan pada model regresi atau klasifikasi apa pun; Namun, metode ini memberikan peningkatan terbesar pada model yang memiliki varian tinggi. Sebagai contoh, model parametrik yang lebih stabil seperti regresi linier dan splines regresi multi-adaptif cenderung kurang mengalami peningkatan dalam kinerja prediksi.

Salah satu manfaat bagging adalah  rata-rata, sampel bootstrap akan berisi 63% (2/3) bagian dari data *training*. Ini menyisakan sekitar 33% (1/3) data dari sampel yang di-bootstrap. Kita dapat menyebutnya sebagai sampel *out-of-bag* (OOB). Kita dapat menggunakan pengamatan OOB untuk memperkirakan akurasi model, menciptakan proses validasi silang alami.

## Validasi Silang

Spesifikasi validasi silang yang digunakan untuk membuat model bagging sama dengan spesifikasi validasi silang model *decission tree*. Perbedaannya adalah pada argumen `trainControl()` tidak ditambahkan argumen `sample`. Hal ini disebabkan pada model bagging yang dibuat ini tidak dilakukan *parameter tuning*.

```{r bag-cv, cache=TRUE}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10,
  # repeats = 5,
  allowParallel = TRUE
)
```

Agumen `method` yang digunakan dalam model ini adalah `"treebag"` yang merupakan `method` yang digunakan jika model bagging yang akan dibuat menggunakan paket `ipred`.

```{r bag-fit, cache=TRUE}
system.time(
bag_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "treebag",
  trControl = cv,
  importance = TRUE
  )
)

bag_fit_cv
```

Proses *training* model berlangsung selama 14.053 detik dengan rata-rata **RMSE** yang diperoleh sebesar 36388.33. Nilai ini merupakan peningkatan dari model *decision trees* yang telah dibuat sebelumnya. 

```{r bag-rmse, cache=TRUE}
bag_rmse <- bag_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

bag_rmse
```

## Model AKhir

Pada tahapan ini, model yang telah di-*training*, diekstrak model akhirnya.

```{r bag-final, cache=TRUE}
bag_fit <- bag_fit_cv$finalModel
```

Untuk melihat performa sebuah model regresi dalam melakukan prediksi, kita dapat melihat plot residu dari model. Untuk melakukannya, jalankan sintaks berikut:

```{r bag-res-vis, cache=TRUE}
pred_train <- predict(bag_fit, baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sc <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point()
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sc, hs, ncol = 2)
```

Pola residu yang terbentuk pada model bagging sama dengan model *decision trees* yang menunjukkan model kesulitan untuk memprediksi nilai `Sale_Price` di atas 400.000.

Adapun performa model bagging pada data baru dapat dicek dengan mengukur nilai **RMSE** model menggunakan data *test*.

```{r bag-rmse-test, cache=TRUE}
# prediksi Sale_Price ames_test
pred_test <- predict(bag_fit, baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```

## Interpretasi Fitur

Untuk melakukan interpretasi terhadap fitur paling berpengaruh dalam model bagging, kita dapat emngetahuinya melalui *varibale importance plot*.

```{r bag-vip, cache=TRUE}
vip(bag_fit_cv, num_features = 10)
```

Berdasarkan hasil plot, terdapat dua buah variabel paling berpengaruh, yaitu: `Gr_Liv_Area` dan `Total_Bsmt_SF`. Untuk melihat efek dari kedua variabel tersebut terhadap prediksi yang dihasilkan model, kita dapat mengetahuinya melalui *patial plot dependencies*.

```{r bag-pdp, cache=TRUE}
p1 <- pdp::partial(bag_fit_cv, pred.var = "Gr_Liv_Area") %>% autoplot()
p2 <- pdp::partial(bag_fit_cv, pred.var = "Total_Bsmt_SF") %>% autoplot()
p3 <- pdp::partial(bag_fit_cv, pred.var = c("Gr_Liv_Area", "Total_Bsmt_SF")) %>% 
  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
              colorkey = TRUE, screen = list(z = -20, x = -60))


gridExtra::grid.arrange(p1, p2, p3, 
                        ncol=2)
```

Berdasarkan output yang dihasilkan, kedua variabel tersebut memiliki relasi non-linier dengan variabel `Sale_Price`. Pola unik yang terbentuk pada plot model bagging yang juga terjadi pada *decision tree* adalah kurva yang terbentuk landai pada awal dan naik secara cepat pada rentang nilai yang pendek dan melandai pada akhir dari rentang nilai variabel.

# Random Forest

Bagging (agregasi bootstrap) adalah teknik yang dapat mengubah model pohon tunggal dengan varian tinggi dan kemampuan prediksi yang buruk menjadi fungsi prediksi yang cukup akurat. Sayangnya, bagging biasanya kekurangan, yiatu: adanya korelasi pada tiap pohon yang mengurangi kinerja keseluruhan model. *Random forest* adalah modifikasi bagging yang membangun koleksi besar pohon yang tidak berkorelasi dan telah menjadi algoritma pembelajaran “out-of-the-box” yang sangat populer yang dengan kinerja prediksi yang baik. 

*Random forest* dibangun di atas prinsip-prinsip dasar yang sama seperti *decision tress* dan bagging. Bagging memperkenalkan komponen acak ke dalam proses pembangunan pohon yang mengurangi varian prediksi pohon tunggal dan meningkatkan kinerja prediksi. Namun, pohon-pohon di bagging tidak sepenuhnya independen satu sama lain karena semua prediktor asli dianggap di setiap split setiap pohon. Sebaliknya, pohon dari sampel bootstrap yang berbeda biasanya memiliki struktur yang mirip satu sama lain (terutama di bagian atas pohon) karena hubungan yang mendasarinya.

Sebagai contoh, jika kita membuat enam pohon keputusan dengan sampel bootstrap data perumahan Boston yang berbeda, kita melihat bahwa puncak pohon semua memiliki struktur yang sangat mirip. Meskipun ada 15 variabel prediktor untuk dipecah, keenam pohon memiliki kedua variabel lstat dan rm yang mendorong beberapa split pertama.

Sebagai contoh, jika kita membuat enam *decision trees* dengan sampel bootstrap [data perumahan Boston](http://uc-r.github.io/(http://lib.stat.cmu.edu/datasets/boston)) yang berbeda, kita melihat bahwa puncak pohon semua memiliki struktur yang sangat mirip. Meskipun ada 15 variabel prediktor untuk dipecah, keenam pohon memiliki kedua variabel `lstat` dan `rm` yang mendorong beberapa split pertama.

![Enam decision trees berdasarkan sampel bootsrap yang berbeda-beda](http://uc-r.github.io/public/images/analytics/random_forests/tree-correlation-1.png)

Karakteristik ini dikenal sebagai **korelasi pohon** dan mencegah bagging dari secara optimal mengurangi varians dari nilai-nilai prediktif. Untuk mengurangi varian lebih lanjut, kita perlu meminimalkan jumlah korelasi antar pohon-pohon tersebut. Ini bisa dicapai dengan menyuntikkan lebih banyak keacakan ke dalam proses penanaman pohon. *Random Forest* mencapai ini dalam dua cara:

1. **Bootstrap**: mirip dengan bagging, setiap pohon ditumbuhkan ke set data *bootstrap resampled*, yang membuatnya berbeda dan agak mendekorelasi antar pohon tersebut.
2. **Split-variable randomization**: setiap kali pemisahan dilakukan, pencarian untuk variabel terbagi terbatas pada subset acak $m$ dari variabel $p$. Untuk pohon regresi, nilai default tipikal adalah $m = p/3$ tetapi ini harus dianggap sebagai *parameter tuning*. Ketika $m = p$, jumlah pengacakan hanya menggunakan langkah 1 dan sama dengan bagging.

Algoritma dasar dari *random forest* adalah sebagai berikut:

```
1.  Diberikan set data training
2.  Pilih jumlah pohon yang akan dibangun (n_trees)
3.  for i = 1 to n_trees do
4.  | Hasilkan sampel bootstrap dari data asli
5.  | Tumbuhkan pohon regresi / klasifikasi ke data yang di-bootstrap
6.  | for each split do
7.  | | Pilih variabel m_try secara acak dari semua variabel p
8.  | | Pilih variabel / titik-split terbaik di antara m_try
9.  | | Membagi node menjadi dua node anak
10. | end
11. | Gunakan kriteria berhenti model pohon biasa untuk menentukan 
    | kapan pohon selesai (tapi jangan pangkas)
12. end
13. Output ensemble of trees 
```

Karena algoritma secara acak memilih sampel bootstrap untuk dilatih dan prediktor digunakan pada setiap split, korelasi pohon akan berkurang melebihi bagging.

## OOB Error vs Test Set Error

Mirip dengan bagging, manfaat alami dari proses *bootstrap resampling* adalah *randomforest* memiliki sampel *out-of-bag* (OOB) yang memberikan perkiraan kesalahan pengujian yang efisien dan masuk akal. Ini memberikan satu set validasi bawaan tanpa kerja ekstra , dan kita tidak perlu mengorbankan data *training* apa pun untuk digunakan untuk validasi. Ini membuat proses identifikasi jumlah pohon yang diperlukan untuk menstabilkan tingkat kesalahan selama proses *tuning* menjadi lebih efisien; Namun, seperti yang diilustrasikan di bawah ini, beberapa perbedaan antara kesalahan OOB dan kesalahan tes diharapkan.

![Random forest OOB vs validation error (Sumber: http://uc-r.github.io/)](http://uc-r.github.io/public/images/analytics/random_forests/oob-error-compare-1.svg)

Selain itu, banyak paket tidak melacak pengamatan mana yang merupakan bagian dari sampel OOB untuk pohon tertentu dan yang tidak. Jika kita membandingkan beberapa model dengan yang lain, kita ingin membuat skor masing-masing pada set validasi yang sama untuk membandingkan kinerja. Selain itu, meskipun secara teknis dimungkinkan untuk menghitung metrik tertentu seperti *root mean squared logarithmic error* (RMSLE) pada sampel OOB, itu tidak dibangun untuk semua paket. Jadi jika kita ingin membandingkan beberapa model atau menggunakan fungsi *loss* yang sedikit lebih tradisional, kita mungkin ingin tetap melakukan validasi silang.

## Kelebihan dan Kekurangan

**Kelbihan**

* Biasanya memiliki kinerja yang sangat bagus
* “*Out-of-the-box*” yang luar biasa bagus - sangat sedikit penyesuaian yang diperlukan
* Kumpulan validasi bawaan - tidak perlu mengorbankan data untuk validasi tambahan
* Tidak diperlukan pra-pemrosesan
* Bersifat *robust* dengan adanya *outlier*

**Kekurangan**

* Dapat menjadi lambat pada set data besar
* Meskipun akurat, seringkali tidak dapat bersaing dengan algoritma *boosting*
* Kurang mudah untuk ditafsirkan


## Validasi Silang dan Parameter Tuning

Pada fungsi `trainControl()` argumen yang digunakan sama dengan model bagging. 

```{r rf-cv, cache=TRUE}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10,
  # repeats = 5,
  allowParallel = TRUE
)
```

Pada proses *training*, kita akan melakukan *parameter tuning* menggunakan metode *grid search*. Parameter yang akan dilakukan tuning pada model ini adalah `mtry` yang merupakan parameter *split-variable randomization*.

```{r rf-grid, cache=TRUE}
hyper_grid <- expand.grid(
  mtry = seq(10, 30, by = 2)
)
```

Pada proses training, `method` yang digunakan adalah `parRF` atau *parallel random forest*. Metode ini memerlukan sejumlah paket tambahan untuk memastikan proses parallel dapat berjalan, seperti: `e1071`, `randomForest`, `plyr`, dan `import`.

```{r rf-fit, cache=TRUE}
# membuat model
system.time(
rf_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "parRF",
  trControl = cv,
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )
)

rf_fit_cv
```

Proses *training* berlangsung selama 1780.221 detik dengan 11 model terbentuk. Dari seluruh model tersebut, model dengan parameter `mtry = 28` memiliki rata-rata **RMSE** yang paling baik. Untuk dapat mengakses **RMSE** model terbaik, jalankan sintaks berikut:

```{r rf-rmse, cache=TRUE}
rf_rmse <- rf_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

rf_rmse
```

Nilai **RMSE** model *random forest* yang dihasilkan jauh lebih baik dibandingkan dua model awal. Reduksi terhadap jumlah pohon yang saling berkorelasi telah meningkatkan performa model secara signifikan.

Berikut adalah ringkasan performa masing-masing model:

```{r rf-vis, chace = TRUE}
# visualisasi
ggplot(rf_fit_cv)
```

## Model AKhir

Untuk mengekstrak model final, jalankan sintaks berikut:

```{r rf-final, cache=TRUE}
rf_fit <- rf_fit_cv$finalModel
```

Untuk mengeceke performa model dalam melakukan prediksi, kita dapat mengecek plot residual model tersebut.

```{r rf-resid-vis, cache=TRUE}
pred_train <- predict(rf_fit, baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sc <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point() 
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sc, hs, ncol = 2)
```

Performa prediksi model mengalami oeningkatan dibanding dua model sebelumnya yang ditunjukkan adanya reduksi dari pola heterkodestisitas pada plot yang dihasilkan.

Untuk mengecek performa prediksi model pada dataset baru (data *test*), jalankan sintaks berikut:

```{r rf-rmse-test, cache=TRUE}
# prediksi Sale_Price ames_test
pred_test <- predict(rf_fit, baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```

## Interpretasi Fitur

Untuk mengetahui variabel apa yang paling berpengaruh terhadap performa model, kita dapat menggunakan visualisasi *variabel importance plot*.

```{r rf-vip, cache=TRUE}
vip(rf_fit, num_features = 10)
```

Berdasarkan visualisasi tersebut, terdapat tiga buah variabel yang memiliki nilai kepentingan yang tinggi, yaitu: `Garage_Cars`, `Year_Built`, dan `Gr_Liv_Area`. Untuk mengetahui efek dari ketiga variabel tersebut terhadap kemampuan prediksi model, jalankan sintaks berikut:

```{r rf-pdp, cache=TRUE}
p1 <- pdp::partial(rf_fit_cv, pred.var = "Garage_Cars") %>% autoplot()
p2 <- pdp::partial(rf_fit_cv, pred.var = "Year_Built") %>% autoplot()
p3 <- pdp::partial(rf_fit_cv, pred.var = "Gr_Liv_Area") %>% autoplot()


gridExtra::grid.arrange(p1, p2, p3, 
                        ncol=2)
```


Berdasarkan output yang dihasilkan, ketiga variabel memiliki relasi non-linier terhadap variabel target.

# Boosting

*Gradient boosted machines* (GBMs) adalah algoritma *machine learning* yang sangat populer yang telah terbukti berhasil di banyak domain dan merupakan salah satu metode utama untuk memenangkan kompetisi Kaggle. Sementara *random forest* membangun ansambel pohon independen yang dalam, GBM membangun ansambel pohon berturut-turut yang dangkal dan lemah dengan setiap pohon belajar dan meningkatkan pada sebelumnya. Ketika digabungkan, banyak pohon berturut-turut yang lemah ini menghasilkan "komite" yang kuat yang seringkali sulit dikalahkan dengan algoritma lain. 

Beberapa model *supervised machine learning* tersusun atas model prediksi tunggal (yaitu [regresi linier](http://uc-r.github.io/linear_regression), [penalized model](http://uc-r.github.io/regularized_regression), [naive bayes](http://uc-r.github.io/naive_bayes), [support vector machines](http://uc-r.github.io/svm)). Atau, pendekatan lain seperti [bagging](http://uc-r.github.io/regression_trees) dan [random forest](http://uc-r.github.io/random_forests) dibangun di atas gagasan membangun ansambel model di mana masing-masing model memprediksi hasil dan kemudian hasil prediksi dirata-rata (regresi) atau menggunakan sistem voting terbanyak (klasifikasi). Keluarga metode *boosting* didasarkan pada strategi konstruktif yang berbeda dari pembentukan ansambel.

Gagasan utama *boosting* adalah menambahkan model-model baru ke ansambel secara berurutan. Pada setiap iterasi tertentu, model pembelajaran dasar-lemah yang baru dilatih sehubungan dengan kesalahan seluruh rangkaian yang dipelajari sejauh ini.

![Pendekatan boosting](http://uc-r.github.io/public/images/analytics/gbm/boosted-trees-process.png)

Mari kita bahas masing-masing komponen kalimat sebelumnya dengan lebih detail karena mereka penting untuk diketahui.

**Base-learning models**: *Boosting* adalah kerangka kerja yang secara iteratif meningkatkan model pembelajaran yang lemah. Banyak aplikasi *gradient boosting* memungkinkan kita untuk "memasukkan" berbagai kelas *weak learner* sesuai keinginan kita Namun dalam praktiknya, algoritma yang ditingkatkan hampir selalu menggunakan *decision trees* sebagai *base learner*-nya. Konsekuensinya, tutorial ini akan membahas *boosting* dalam konteks pohon regresi atau klasifikasi.

**Training weak models**: *Weak model* adalah model yang tingkat kesalahannya hanya sedikit lebih baik daripada menebak secara acak. Gagasan di balik *boosting* adalah bahwa setiap model berurutan membangun model lemah sederhana untuk sedikit memperbaiki kesalahan yang tersisa. Sehubungan dengan *decision trees*, pohon dangkal mewakili *weak learner*. Umumnya, pohon dengan hanya 1-6 pohon digunakan. Menggabungkan banyak *weak model* (versus yang kuat) memiliki beberapa manfaat:

* *Speed*: Membangun model lemah adalah murah secara proses komputasi
* *Accuracy improvement*: *Weak model* memungkinkan algoritma untuk belajar secara lambat; melakukan penyesuaian kecil di area baru yang kinerjanya tidak baik. Secara umum, pendekatan statistik *weak learner* cenderung berkinerja baik.
* *Avoids overfitting*: Karena hanya membuat perbaikan bertahap kecil dengan masing-masing model dalam ansambel, ini memungkinkan kita untuk menghentikan proses pembelajaran segera setelah overfitting telah terdeteksi (biasanya dengan menggunakan validasi silang).

**Sequential training with respect to errors**: *Boosted trees* ditumbuhkan secara berurutan; setiap pohon ditumbuhkan menggunakan informasi dari pohon yang sebelumnya ditumbuhkan. Algoritma dasar untuk *boosted model* dapat digeneralisasi ke yang berikut ini di mana $x$ mewakili fitur data dan variabel $y$ mewakili respons:

1. Buat *decision trees* pada data: $F_1\left(x\right)=y$
2. Buat *decision trees* selanjutnya menggunakan data residual dari *decision trees* sebelumnya: $h_1\left(x\right)=y-F_1\left(x\right)$.
3. Tambahkan pohon baru tersebut ke dalam algoritma: $F_2\left(x\right)= F_1\left(x\right)+h_1\left(x\right)$
4. Buat *decision trees* baru pada residu $F_2$: $h_2\left(x\right)=y - F_2\left(x\right)$
5. Tambahkan *decision trees* tersebut ke dalam algoritma: $F_3\left(x\right)=F_2\left(x\right)+h_2\left(x\right)$
6. Lanjutkan proses tersebut hingga sebuah mekanisme (biasanya hasil validasi silang) menyatakan proses tersebut harus berhenti.

## Gradient Descent

Banyak algoritma, termasuk pohon keputusan, fokus pada meminimalkan residu dan oleh karena itu, menekankan fungsi *loss* MSE. Algoritma yang dibahas pada bagian sebelumnya menguraikan pendekatan *sequantial decision trees fitting* untuk meminimalkan kesalahan. Pendekatan khusus ini adalah bagaimana meningkatkan gradien meminimalkan fungsi *loss* *mean squared error* (MSE). Namun, seringkali kita ingin fokus pada fungsi *loss* lainnya seperti *mean absolute error* (MAE) atau untuk dapat menerapkan metode ini ke masalah klasifikasi dengan fungsi *loss* seperti *deviance*. *Gradient boosting machine*  berasal dari fakta bahwa prosedur ini dapat digeneralisasi ke fungsi *loss* selain MSE.

*Gradient Boosting* dianggap sebagai algoritma *gradient descent*. *Gradient descent* adalah algoritma optimasi yang sangat umum yang mampu menemukan solusi optimal untuk berbagai masalah. Gagasan umum *gradient descent* adalah mengubah parameter secara iteratif untuk meminimalkan fungsi *cost*. Misalkan kita adalah pemain ski menuruni bukit dan berpacu dengan teman kita. Strategi yang baik untuk mengalahkan teman kita ke bawah adalah mengambil jalan setapak dengan kemiringan paling curam. Inilah yang dilakukan oleh *gradient descent* - ini mengukur gradient lokal dari fungsi *loss* (*cost*) untuk sekumpulan parameter ($\Theta$) dan mengambil langkah-langkah ke arah gradien yang menurun. Setelah gradien nol, kita telah mencapai minimum.


![Gradient descent (Sumber: Geron, 2017)](http://uc-r.github.io/public/images/analytics/gbm/gradient_descent.png)

Gradient descent dapat dilakukan pada setiap fungsi *loss* yang dapat diturunkan (*differentiable*).  Akibatnya, ini memungkinkan GBM untuk mengoptimalkan berbagai fungsi *loss* seperti yang diinginkan. Parameter penting dalam *gradient descent* adalah *step size* yang ditentukan oleh *learning rate*. Jika *learning rate* terlalu kecil, maka algoritma akan mengambil banyak iterasi untuk menemukan minimum. Di sisi lain, jika tingkat pembelajaran terlalu tinggi, kita mungkin melewati batas minimum dan berakhir lebih jauh daripada saat kita mulai.

![Perbandinga learning rate (Sumber: Geron, 2017)](http://uc-r.github.io/public/images/analytics/gbm/learning_rate_comparison.png)

Selain itu, tidak semua fungsi *cost* bersifat *covex* (berbentuk mangkuk). Mungkin ada *local minimas*, *plateaus*, dan medan tidak teratur lainnya dari fungsi *loss* yang membuat sulit menemukan minimum global. *Stochastic gradient descent* dapat membantu kita mengatasi masalah ini dengan mengambil sampel sebagian kecil dari pengamatan pelatihan (biasanya tanpa penggantian) dan menumbuhkan pohon berikutnya menggunakan subsampel itu. Ini membuat algoritma lebih cepat tetapi sifat stokastik dari random sampling juga menambahkan beberapa sifat acak dalam menuruni gradien fungsi *loss*. Meskipun keacakan ini tidak memungkinkan algoritma untuk menemukan minimum global absolut, itu sebenarnya dapat membantu algoritma melompat keluar dari minimum lokal dan mematikan *plateus* dan mendekati minimum global.

![Stocahstic gradient descent (Sumber: Geron, 2017)](http://uc-r.github.io/public/images/analytics/gbm/stochastic_gradient_descent.png)


## Validasi Silang dan Parameter Tuning

```{r boost-cv, cache=TRUE}
# spesifikasi metode validasi silang
cv <- trainControl(
  # possible value: "boot", "boot632", "optimism_boot", "boot_all", "cv", 
  #                 "repeatedcv", "LOOCV", "LGOCV"
  method = "cv", 
  number = 10, 
  # repeats = 5,
  allowParallel = TRUE
)
```

```{r boost-grid, cache=TRUE}
hyper_grid <- expand.grid(
  n.trees = 6000,
  shrinkage = 0.01,
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 10, 15)
)
```

```{r boost-fit, cache=TRUE}
system.time(
gb_fit_cv <- train(
  blueprint,
  data = ames_train,
  method = "gbm",
  trControl = cv,
  tuneGrid = hyper_grid,
  verbose = FALSE,
  metric = "RMSE"
  )
)

gb_fit_cv
```

```{r boost-rmse, cache=TRUE}
boost_rmse <- gb_fit_cv$results %>%
  arrange(RMSE) %>%
  slice(1) %>%
  select(RMSE) %>% pull()

boost_rmse
```

```{r boost-vis, cache=TRUE}
ggplot(gb_fit_cv)
```


## Model Akhir

```{r boost-final, cache=TRUE}
gb_fit <- gb_fit_cv$finalModel
```


```{r boost-resid, cache=TRUE}
pred_train <- predict(gb_fit, n.trees = gb_fit$n.trees,
                      baked_train)
residual <- mutate(baked_train, residual = Sale_Price - pred_train)

# resiual vs actual
sc <- ggplot(residual, aes(x = Sale_Price, y = residual)) +
  geom_point() 
# residual distribution
hs <- ggplot(residual, aes(x = residual)) +
  geom_histogram()

gridExtra::grid.arrange(sp, hs, ncol = 2)
```

```{r boost-test-rmse, cache=TRUE}
# prediksi Sale_Price ames_test
pred_test <- predict(gb_fit, n.trees = gb_fit$n.trees,
                     baked_test)

## RMSE
RMSE(pred_test, baked_test$Sale_Price, na.rm = TRUE)
```

## Tingkat Kepentingan Variabel

```{r boost-vip, cache=TRUE}
vip(gb_fit_cv, num_features = 10)
```




# Ringkasan Model


```{r table, cache=TRUE}
table <- tibble(Model = c("Decision Tree", "Bagging", "Random Forest", "Gradient Boosting"),
                RMSE = c(dt_rmse, bag_rmse, rf_rmse, boost_rmse))

table
```


